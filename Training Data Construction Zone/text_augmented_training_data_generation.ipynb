{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50b376f-0c25-44e1-8fe2-63a4f07231fa",
   "metadata": {},
   "source": [
    "## Training Data Generation (Augmented Text Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1513906-5c7c-4525-abc1-0067f560c81f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (2.32.5)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nlpaug) (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Installing collected packages: nlpaug\n",
      "Successfully installed nlpaug-1.1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sakurasnow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install nlpaug\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7eba5bec-aa97-45d6-ad6a-492a2e432f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SakuraSnow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing useful dependencies\n",
    "import re\n",
    "import io\n",
    "import nltk\n",
    "import torch\n",
    "import boto3\n",
    "import random\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Download the corpus\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 10721\n",
    "random.seed(SEED) \n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6150ca19-7cf1-4cba-b9ff-ffd20cfd4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053b671e-4df8-4153-a454-cd1e1000b35b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'training-data-construction-zone' already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '187A5C03F502FBA5',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'AAAAAA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '187A5C03F502FBA5',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2101',\n",
       "   'x-ratelimit-remaining': '2101',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Sat, 22 Nov 2025 14:56:16 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ChecksumCRC32': 'AAAAAA==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new Bucket in Min-IO to store our augmented training data\n",
    "\n",
    "# List existing buckets\n",
    "buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n",
    "\n",
    "# Function that given a name, creates a bucket\n",
    "def createBucket(name, list_buckets):\n",
    "    if name in list_buckets:\n",
    "        print(f\"Bucket '{name}' already exists!\")\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=name)\n",
    "        print(f\"Created bucket: {name}\")\n",
    "\n",
    "# Create a bucket named landing_zone\n",
    "createBucket(\"training-data-construction-zone\", buckets)\n",
    "# Sub-bucket: Baseline Training Data\n",
    "s3.put_object(Bucket=\"training-data-construction-zone\", Key=\"text_augmented-training-data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf232f2c-e390-4504-8031-466b64f9eca5",
   "metadata": {},
   "source": [
    "In this notebook, we will apply various data augmentation techniques to increase the diversity of our text data and enhance the variance in the training set. Specifically, we will implement four techniques: **random word deletion**, **random word swap**, **random spelling error**, and **random synonym replacement**.\n",
    "\n",
    "- **Random word deletion** removes a word from the text with a certain probability `p`.  \n",
    "- **Random word swap** is similar, but instead of deleting words, it randomly changes the positions of words in the text.  \n",
    "- **Random spelling error** introduces a spelling mistake in a word with probability `p`.  \n",
    "- **Random synonym replacement** replaces a word with one of its synonyms with probability `p`, where synonyms are obtained from the WordNet corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "de94296d-7457-4de7-828d-4f4c01f1fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hatsune Miku is a virtual pop star and vocaloid software persona created by Crypton Future Media. Represented as a 16-year-old girl with long turquoise twin-tails, she 'sings' by synthesizing voices from the Vocaloid engine, allowing producers to create original songs. Since her debut in 2007, Miku has gained a massive global following, performing in live concerts as a hologram and appearing in video games, merchandise, and collaborations, making her a symbol of digital music culture.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use a sample description generated using Chat-GPT as an example\n",
    "samp_text = \"Hatsune Miku is a virtual pop star and vocaloid software persona created by Crypton Future Media. Represented as a 16-year-old girl with long turquoise twin-tails, she 'sings' by synthesizing voices from the Vocaloid engine, allowing producers to create original songs. Since her debut in 2007, Miku has gained a massive global following, performing in live concerts as a hologram and appearing in video games, merchandise, and collaborations, making her a symbol of digital music culture.\"\n",
    "samp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b2e0aa4b-ad44-46bf-bb36-8e8a9e853e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hatsune',\n",
       " 'Miku',\n",
       " 'is',\n",
       " 'a',\n",
       " 'virtual',\n",
       " 'pop',\n",
       " 'star',\n",
       " 'and',\n",
       " 'vocaloid',\n",
       " 'software',\n",
       " 'persona',\n",
       " 'created',\n",
       " 'by',\n",
       " 'Crypton',\n",
       " 'Future',\n",
       " 'Media',\n",
       " '.',\n",
       " 'Represented',\n",
       " 'as',\n",
       " 'a',\n",
       " '16',\n",
       " '-',\n",
       " 'year',\n",
       " '-',\n",
       " 'old',\n",
       " 'girl',\n",
       " 'with',\n",
       " 'long',\n",
       " 'turquoise',\n",
       " 'twin',\n",
       " '-',\n",
       " 'tails',\n",
       " ',',\n",
       " 'she',\n",
       " \"'\",\n",
       " 'sings',\n",
       " \"'\",\n",
       " 'by',\n",
       " 'synthesizing',\n",
       " 'voices',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Vocaloid',\n",
       " 'engine',\n",
       " ',',\n",
       " 'allowing',\n",
       " 'producers',\n",
       " 'to',\n",
       " 'create',\n",
       " 'original',\n",
       " 'songs',\n",
       " '.',\n",
       " 'Since',\n",
       " 'her',\n",
       " 'debut',\n",
       " 'in',\n",
       " '2007',\n",
       " ',',\n",
       " 'Miku',\n",
       " 'has',\n",
       " 'gained',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'global',\n",
       " 'following',\n",
       " ',',\n",
       " 'performing',\n",
       " 'in',\n",
       " 'live',\n",
       " 'concerts',\n",
       " 'as',\n",
       " 'a',\n",
       " 'hologram',\n",
       " 'and',\n",
       " 'appearing',\n",
       " 'in',\n",
       " 'video',\n",
       " 'games',\n",
       " ',',\n",
       " 'merchandise',\n",
       " ',',\n",
       " 'and',\n",
       " 'collaborations',\n",
       " ',',\n",
       " 'making',\n",
       " 'her',\n",
       " 'a',\n",
       " 'symbol',\n",
       " 'of',\n",
       " 'digital',\n",
       " 'music',\n",
       " 'culture',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple tokenizer to split the text into a list of tokens (words in this case)\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "# Test the simple tokenizer of words\n",
    "simple_tokenize(samp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e218c48-fadc-4814-8e76-90d4d3d58bf2",
   "metadata": {},
   "source": [
    "In the following cell we define the methods we described previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "beba5fcb-2c1a-4c49-8adc-ce9e8ac759e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### Random word deletion ####\n",
    "###############################\n",
    "\n",
    "# Delete each token with probability p.\n",
    "def random_deletion(tokens: List[str], p: float = 0.1) -> List[str]:\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "\n",
    "    kept = [t for t in tokens if random.random() > p]\n",
    "    if not kept:\n",
    "        kept.append(random.choice(tokens))\n",
    "    return kept\n",
    "\n",
    "###########################\n",
    "##### Random word swap ####\n",
    "###########################\n",
    "\n",
    "# Randomly swap a small portion of tokens.\n",
    "def random_swap(tokens: List[str], ratio: float = 0.05) -> list[str]:\n",
    "    n = len(tokens)\n",
    "    if n < 2:\n",
    "        return tokens\n",
    "\n",
    "    n_swaps = max(1, int(ratio * n))\n",
    "\n",
    "    for _ in range(n_swaps):\n",
    "        i, j = random.sample(range(n), 2)\n",
    "        tokens[i], tokens[j] = tokens[j], tokens[i]\n",
    "    return tokens\n",
    "\n",
    "################################\n",
    "##### Random spelling error ####\n",
    "################################\n",
    "\n",
    "# Introduce a simple spelling error in a single word\n",
    "def corrupt_word(word: str) -> str:\n",
    "    if len(word) == 0:\n",
    "        return word\n",
    "\n",
    "    ALPHABET = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "    op = random.choice([\"delete\", \"substitute\", \"duplicate\"])\n",
    "\n",
    "    if op == \"delete\" and len(word) > 1:\n",
    "        pos = random.randrange(len(word))\n",
    "        return word[:pos] + word[pos+1:]\n",
    "\n",
    "    if op == \"substitute\":\n",
    "        pos = random.randrange(len(word))\n",
    "        new_char = random.choice(ALPHABET)\n",
    "        return word[:pos] + new_char + word[pos+1:]\n",
    "\n",
    "    if op == \"duplicate\":\n",
    "        pos = random.randrange(len(word))\n",
    "        return word[:pos] + word[pos] + word[pos:]\n",
    "\n",
    "    return word\n",
    "\n",
    "# For each alphabetical token, apply a spelling error with probability p.\n",
    "def random_spelling_error(tokens: List[str], p: float = 0.1) -> List[str]:\n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.isalpha() and random.random() < p:\n",
    "            new_tokens.append(corrupt_word(t))\n",
    "        else:\n",
    "            new_tokens.append(t)\n",
    "    return new_tokens\n",
    "\n",
    "######################################\n",
    "##### Random synonym replacement #####\n",
    "######################################\n",
    "\n",
    "# Collect synonyms from WordNet\n",
    "def get_synonyms(word: str) -> List[str]:\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            lemma_name = lemma.name().replace(\"_\", \" \")\n",
    "            if lemma_name.lower() != word.lower():\n",
    "                synonyms.add(lemma_name)\n",
    "    return list(synonyms)\n",
    "\n",
    "# Randomly choose a small portion of tokens and replace them with synonyms.\n",
    "def random_synonym_replacement(tokens: List[str], ratio: float = 0.05) -> List[str]:\n",
    "    n = len(tokens)\n",
    "    n_replacements = max(1, int(ratio * n))\n",
    "    candidate_indices = [\n",
    "        i for i, t in enumerate(tokens)\n",
    "        if t.isalpha() and len(t) > 2 # skip punctuation & very short tokens\n",
    "    ]\n",
    "    if not candidate_indices:\n",
    "        return tokens\n",
    "\n",
    "    indices = random.sample(candidate_indices, n_replacements)\n",
    "\n",
    "    for idx in indices:\n",
    "        word = tokens[idx]\n",
    "        syns = get_synonyms(word)\n",
    "        if syns:\n",
    "            tokens[idx] = random.choice(syns)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "256ce462-f008-43f7-871a-beb8d9a39307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hatsune Miku is a virtual pop star and vocaloid software persona created Crypton Future Media . Represented as 16 - year - old girl with long turquoise twin - tails , she ' sings ' by synthesizing voices from the Vocaloid engine , producers to create original songs . Since her debut in 2007 , has gained a massive following , performing in live concerts a hologram and appearing video games , merchandise and collaborations , making her a symbol of digital .\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Random word deletion method\n",
    "\" \".join(random_deletion(simple_tokenize(samp_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ea01622-8117-4000-95a3-a73478b4bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hatsune hologram is a virtual pop star , vocaloid software persona created by Crypton Future Media . Represented as a and - year - old girl with long turquoise twin - tails , she ' sings ' by synthesizing voices from the Vocaloid engine , allowing producers to create original songs . Since her debut in 2007 and Miku has gained a massive global following , performing in live concerts video a Miku 16 appearing in as games , merchandise , and collaborations , making her a symbol of digital music culture .\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Random word swap method\n",
    "\" \".join(random_swap(simple_tokenize(samp_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2c1eddb-6aff-4943-ada9-6a54a1c549b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hatsune kiku is a virtual pop star annd vocaloid software persona created by Crypton Future Mxdia . Represented as a 16 - year - old girl with long turquoise twin - tails , she ' sings ' by synthesizing voices from the Vocaloid engine , allowing producers to create original songs . Since hgr debut in 2007 , Miku has gainek a massive global following , performing in live concerts as a hologram and appearing in video games , merchandise , and collaborations , maing her a symbol of digital music culture .\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Random spelling error method\n",
    "\" \".join(random_spelling_error(simple_tokenize(samp_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7ae79863-0b84-4ac8-b3c2-16cdaaa9b4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hatsune Miku is a virtual pop star and vocaloid software role created by Crypton Future Media . Represented as a 16 - year - old girl with long turquoise twin - tails , she ' sings ' by synthesizing voices from the Vocaloid engine , allowing producers to create original songs . Since her debut in 2007 , Miku has gained a massive global following , performing in exist concerts as a hologram and appearing in TV games , merchandise , and collaborations , making her a symbol of digital music culture .\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Random synonym replacement method\n",
    "\" \".join(random_synonym_replacement(simple_tokenize(samp_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5caf8a-0e06-4e27-ab6f-5c6513cb0fc0",
   "metadata": {},
   "source": [
    "Now that we have evaluated the performance of the output texts, in the following cells we will create an augmented text file for each text file in the baseline training data. The corresponding image file will remain the same, but we will make a copy of it so that it shares the same prefix as the augmented text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c61aab21-7382-4a57-98e5-281e893fd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this function to retrieve an text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4c5bc29d-ef67-4156-a00e-aa30dc3c7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates augmented data for our descriptions in the baseline training data\n",
    "def text_augmentation(src_bucket, dest_bucket, dest_prefix=\"text_augmented-training-data/\"):\n",
    "    \n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=\"baseline-training-data/\"):\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "\n",
    "            # Add new prefixes\n",
    "            key_1 = dest_prefix + \"rwd\" + \"_\" + key.split(\"/\")[1]\n",
    "            key_2 = dest_prefix + \"rws\" + \"_\" + key.split(\"/\")[1]\n",
    "            key_3 = dest_prefix + \"rse\" + \"_\" + key.split(\"/\")[1]\n",
    "            key_4 = dest_prefix + \"rsr\" + \"_\" + key.split(\"/\")[1]\n",
    "\n",
    "            # New key for original text and image file\n",
    "            new_key = dest_prefix + key.split(\"/\")[1]\n",
    "\n",
    "            if \"image\" in key:\n",
    "\n",
    "                # Copy objects without top-level folder and rename them\n",
    "                copy_source_image = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=new_key, CopySource=copy_source_image) # Original image\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=key_1, CopySource=copy_source_image) # Image for the first augemented text\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=key_2, CopySource=copy_source_image) # Image for the second augemented text\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=key_3, CopySource=copy_source_image) # Image for the third augemented text\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=key_4, CopySource=copy_source_image) # Image for the fourth augemented text\n",
    "                \n",
    "            elif \"text\" in key:\n",
    "\n",
    "                # Get the description\n",
    "                description = get_text(src_bucket, key)\n",
    "\n",
    "                # Get augmented descriptions\n",
    "                str1 = \" \".join(random_deletion(simple_tokenize(description)))\n",
    "                str2 = \" \".join(random_swap(simple_tokenize(description)))\n",
    "                str3 = \" \".join(random_spelling_error(simple_tokenize(description)))\n",
    "                str4 = \" \".join(random_synonym_replacement(simple_tokenize(description)))\n",
    "\n",
    "                # Copy objects without top-level folder and rename them\n",
    "                copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=new_key, CopySource=copy_source_text)\n",
    "                s3.put_object(Bucket=dest_bucket, Key=key_1, Body=io.BytesIO(str1.encode(\"utf-8\")),ContentType=\"text/plain\")\n",
    "                s3.put_object(Bucket=dest_bucket, Key=key_2, Body=io.BytesIO(str2.encode(\"utf-8\")),ContentType=\"text/plain\")\n",
    "                s3.put_object(Bucket=dest_bucket, Key=key_3, Body=io.BytesIO(str3.encode(\"utf-8\")),ContentType=\"text/plain\")\n",
    "                s3.put_object(Bucket=dest_bucket, Key=key_4, Body=io.BytesIO(str4.encode(\"utf-8\")),ContentType=\"text/plain\")\n",
    "\n",
    "                print(f\"✅ Augmented data for #{key.split('/')[1]} created successfully.\")\n",
    "\n",
    "    print(f\"✅ All augmented text data have been successfully uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0ca68a8f-e457-433c-a53c-f6e40b3bbcf5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Augmented data for #text_000001.txt created successfully.\n",
      "✅ Augmented data for #text_000002.txt created successfully.\n",
      "✅ Augmented data for #text_000003.txt created successfully.\n",
      "✅ Augmented data for #text_000004.txt created successfully.\n",
      "✅ Augmented data for #text_000005.txt created successfully.\n",
      "✅ Augmented data for #text_000006.txt created successfully.\n",
      "✅ Augmented data for #text_000007.txt created successfully.\n",
      "✅ Augmented data for #text_000008.txt created successfully.\n",
      "✅ Augmented data for #text_000009.txt created successfully.\n",
      "✅ Augmented data for #text_000010.txt created successfully.\n",
      "✅ Augmented data for #text_000011.txt created successfully.\n",
      "✅ Augmented data for #text_000012.txt created successfully.\n",
      "✅ Augmented data for #text_000013.txt created successfully.\n",
      "✅ Augmented data for #text_000014.txt created successfully.\n",
      "✅ Augmented data for #text_000015.txt created successfully.\n",
      "✅ Augmented data for #text_000016.txt created successfully.\n",
      "✅ Augmented data for #text_000017.txt created successfully.\n",
      "✅ Augmented data for #text_000018.txt created successfully.\n",
      "✅ Augmented data for #text_000019.txt created successfully.\n",
      "✅ Augmented data for #text_000020.txt created successfully.\n",
      "✅ Augmented data for #text_000021.txt created successfully.\n",
      "✅ Augmented data for #text_000022.txt created successfully.\n",
      "✅ Augmented data for #text_000023.txt created successfully.\n",
      "✅ Augmented data for #text_000024.txt created successfully.\n",
      "✅ Augmented data for #text_000025.txt created successfully.\n",
      "✅ Augmented data for #text_000026.txt created successfully.\n",
      "✅ Augmented data for #text_000027.txt created successfully.\n",
      "✅ Augmented data for #text_000028.txt created successfully.\n",
      "✅ Augmented data for #text_000029.txt created successfully.\n",
      "✅ Augmented data for #text_000030.txt created successfully.\n",
      "✅ Augmented data for #text_000031.txt created successfully.\n",
      "✅ Augmented data for #text_000032.txt created successfully.\n",
      "✅ Augmented data for #text_000033.txt created successfully.\n",
      "✅ Augmented data for #text_000034.txt created successfully.\n",
      "✅ Augmented data for #text_000035.txt created successfully.\n",
      "✅ Augmented data for #text_000036.txt created successfully.\n",
      "✅ Augmented data for #text_000037.txt created successfully.\n",
      "✅ Augmented data for #text_000038.txt created successfully.\n",
      "✅ Augmented data for #text_000039.txt created successfully.\n",
      "✅ Augmented data for #text_000040.txt created successfully.\n",
      "✅ Augmented data for #text_000041.txt created successfully.\n",
      "✅ Augmented data for #text_000042.txt created successfully.\n",
      "✅ Augmented data for #text_000043.txt created successfully.\n",
      "✅ Augmented data for #text_000044.txt created successfully.\n",
      "✅ Augmented data for #text_000045.txt created successfully.\n",
      "✅ Augmented data for #text_000046.txt created successfully.\n",
      "✅ Augmented data for #text_000047.txt created successfully.\n",
      "✅ Augmented data for #text_000048.txt created successfully.\n",
      "✅ Augmented data for #text_000049.txt created successfully.\n",
      "✅ Augmented data for #text_000050.txt created successfully.\n",
      "✅ Augmented data for #text_000051.txt created successfully.\n",
      "✅ Augmented data for #text_000052.txt created successfully.\n",
      "✅ Augmented data for #text_000053.txt created successfully.\n",
      "✅ Augmented data for #text_000054.txt created successfully.\n",
      "✅ Augmented data for #text_000055.txt created successfully.\n",
      "✅ Augmented data for #text_000056.txt created successfully.\n",
      "✅ Augmented data for #text_000057.txt created successfully.\n",
      "✅ Augmented data for #text_000058.txt created successfully.\n",
      "✅ Augmented data for #text_000059.txt created successfully.\n",
      "✅ Augmented data for #text_000060.txt created successfully.\n",
      "✅ Augmented data for #text_000061.txt created successfully.\n",
      "✅ Augmented data for #text_000062.txt created successfully.\n",
      "✅ Augmented data for #text_000063.txt created successfully.\n",
      "✅ Augmented data for #text_000064.txt created successfully.\n",
      "✅ Augmented data for #text_000065.txt created successfully.\n",
      "✅ Augmented data for #text_000066.txt created successfully.\n",
      "✅ Augmented data for #text_000067.txt created successfully.\n",
      "✅ Augmented data for #text_000068.txt created successfully.\n",
      "✅ Augmented data for #text_000069.txt created successfully.\n",
      "✅ Augmented data for #text_000070.txt created successfully.\n",
      "✅ Augmented data for #text_000071.txt created successfully.\n",
      "✅ Augmented data for #text_000072.txt created successfully.\n",
      "✅ Augmented data for #text_000073.txt created successfully.\n",
      "✅ Augmented data for #text_000074.txt created successfully.\n",
      "✅ Augmented data for #text_000075.txt created successfully.\n",
      "✅ Augmented data for #text_000076.txt created successfully.\n",
      "✅ Augmented data for #text_000077.txt created successfully.\n",
      "✅ Augmented data for #text_000078.txt created successfully.\n",
      "✅ Augmented data for #text_000079.txt created successfully.\n",
      "✅ Augmented data for #text_000080.txt created successfully.\n",
      "✅ Augmented data for #text_000081.txt created successfully.\n",
      "✅ Augmented data for #text_000082.txt created successfully.\n",
      "✅ Augmented data for #text_000083.txt created successfully.\n",
      "✅ Augmented data for #text_000084.txt created successfully.\n",
      "✅ Augmented data for #text_000085.txt created successfully.\n",
      "✅ Augmented data for #text_000086.txt created successfully.\n",
      "✅ Augmented data for #text_000087.txt created successfully.\n",
      "✅ Augmented data for #text_000088.txt created successfully.\n",
      "✅ Augmented data for #text_000089.txt created successfully.\n",
      "✅ Augmented data for #text_000090.txt created successfully.\n",
      "✅ Augmented data for #text_000091.txt created successfully.\n",
      "✅ Augmented data for #text_000092.txt created successfully.\n",
      "✅ Augmented data for #text_000093.txt created successfully.\n",
      "✅ Augmented data for #text_000094.txt created successfully.\n",
      "✅ Augmented data for #text_000095.txt created successfully.\n",
      "✅ Augmented data for #text_000096.txt created successfully.\n",
      "✅ Augmented data for #text_000097.txt created successfully.\n",
      "✅ Augmented data for #text_000098.txt created successfully.\n",
      "✅ Augmented data for #text_000099.txt created successfully.\n",
      "✅ Augmented data for #text_000100.txt created successfully.\n",
      "✅ Augmented data for #text_000101.txt created successfully.\n",
      "✅ Augmented data for #text_000102.txt created successfully.\n",
      "✅ Augmented data for #text_000103.txt created successfully.\n",
      "✅ Augmented data for #text_000104.txt created successfully.\n",
      "✅ Augmented data for #text_000105.txt created successfully.\n",
      "✅ Augmented data for #text_000106.txt created successfully.\n",
      "✅ Augmented data for #text_000107.txt created successfully.\n",
      "✅ Augmented data for #text_000108.txt created successfully.\n",
      "✅ Augmented data for #text_000109.txt created successfully.\n",
      "✅ Augmented data for #text_000110.txt created successfully.\n",
      "✅ Augmented data for #text_000111.txt created successfully.\n",
      "✅ Augmented data for #text_000112.txt created successfully.\n",
      "✅ Augmented data for #text_000113.txt created successfully.\n",
      "✅ Augmented data for #text_000114.txt created successfully.\n",
      "✅ Augmented data for #text_000115.txt created successfully.\n",
      "✅ Augmented data for #text_000116.txt created successfully.\n",
      "✅ Augmented data for #text_000117.txt created successfully.\n",
      "✅ Augmented data for #text_000118.txt created successfully.\n",
      "✅ Augmented data for #text_000119.txt created successfully.\n",
      "✅ Augmented data for #text_000120.txt created successfully.\n",
      "✅ Augmented data for #text_000121.txt created successfully.\n",
      "✅ Augmented data for #text_000122.txt created successfully.\n",
      "✅ Augmented data for #text_000123.txt created successfully.\n",
      "✅ Augmented data for #text_000124.txt created successfully.\n",
      "✅ Augmented data for #text_000125.txt created successfully.\n",
      "✅ Augmented data for #text_000126.txt created successfully.\n",
      "✅ Augmented data for #text_000127.txt created successfully.\n",
      "✅ Augmented data for #text_000128.txt created successfully.\n",
      "✅ Augmented data for #text_000129.txt created successfully.\n",
      "✅ Augmented data for #text_000130.txt created successfully.\n",
      "✅ Augmented data for #text_000131.txt created successfully.\n",
      "✅ Augmented data for #text_000132.txt created successfully.\n",
      "✅ Augmented data for #text_000133.txt created successfully.\n",
      "✅ Augmented data for #text_000134.txt created successfully.\n",
      "✅ Augmented data for #text_000135.txt created successfully.\n",
      "✅ Augmented data for #text_000136.txt created successfully.\n",
      "✅ Augmented data for #text_000137.txt created successfully.\n",
      "✅ Augmented data for #text_000138.txt created successfully.\n",
      "✅ Augmented data for #text_000139.txt created successfully.\n",
      "✅ Augmented data for #text_000140.txt created successfully.\n",
      "✅ Augmented data for #text_000141.txt created successfully.\n",
      "✅ Augmented data for #text_000142.txt created successfully.\n",
      "✅ Augmented data for #text_000143.txt created successfully.\n",
      "✅ Augmented data for #text_000144.txt created successfully.\n",
      "✅ Augmented data for #text_000145.txt created successfully.\n",
      "✅ Augmented data for #text_000146.txt created successfully.\n",
      "✅ Augmented data for #text_000147.txt created successfully.\n",
      "✅ Augmented data for #text_000148.txt created successfully.\n",
      "✅ Augmented data for #text_000149.txt created successfully.\n",
      "✅ Augmented data for #text_000150.txt created successfully.\n",
      "✅ Augmented data for #text_000151.txt created successfully.\n",
      "✅ Augmented data for #text_000152.txt created successfully.\n",
      "✅ Augmented data for #text_000153.txt created successfully.\n",
      "✅ Augmented data for #text_000154.txt created successfully.\n",
      "✅ Augmented data for #text_000155.txt created successfully.\n",
      "✅ Augmented data for #text_000156.txt created successfully.\n",
      "✅ Augmented data for #text_000157.txt created successfully.\n",
      "✅ Augmented data for #text_000158.txt created successfully.\n",
      "✅ Augmented data for #text_000159.txt created successfully.\n",
      "✅ Augmented data for #text_000160.txt created successfully.\n",
      "✅ Augmented data for #text_000161.txt created successfully.\n",
      "✅ Augmented data for #text_000162.txt created successfully.\n",
      "✅ Augmented data for #text_000163.txt created successfully.\n",
      "✅ Augmented data for #text_000164.txt created successfully.\n",
      "✅ Augmented data for #text_000165.txt created successfully.\n",
      "✅ Augmented data for #text_000166.txt created successfully.\n",
      "✅ Augmented data for #text_000167.txt created successfully.\n",
      "✅ Augmented data for #text_000168.txt created successfully.\n",
      "✅ Augmented data for #text_000169.txt created successfully.\n",
      "✅ Augmented data for #text_000170.txt created successfully.\n",
      "✅ Augmented data for #text_000171.txt created successfully.\n",
      "✅ Augmented data for #text_000172.txt created successfully.\n",
      "✅ Augmented data for #text_000173.txt created successfully.\n",
      "✅ Augmented data for #text_000174.txt created successfully.\n",
      "✅ Augmented data for #text_000175.txt created successfully.\n",
      "✅ Augmented data for #text_000176.txt created successfully.\n",
      "✅ Augmented data for #text_000177.txt created successfully.\n",
      "✅ Augmented data for #text_000178.txt created successfully.\n",
      "✅ Augmented data for #text_000179.txt created successfully.\n",
      "✅ Augmented data for #text_000180.txt created successfully.\n",
      "✅ Augmented data for #text_000181.txt created successfully.\n",
      "✅ Augmented data for #text_000182.txt created successfully.\n",
      "✅ Augmented data for #text_000183.txt created successfully.\n",
      "✅ Augmented data for #text_000184.txt created successfully.\n",
      "✅ Augmented data for #text_000185.txt created successfully.\n",
      "✅ Augmented data for #text_000186.txt created successfully.\n",
      "✅ Augmented data for #text_000187.txt created successfully.\n",
      "✅ Augmented data for #text_000188.txt created successfully.\n",
      "✅ Augmented data for #text_000189.txt created successfully.\n",
      "✅ Augmented data for #text_000190.txt created successfully.\n",
      "✅ Augmented data for #text_000191.txt created successfully.\n",
      "✅ Augmented data for #text_000192.txt created successfully.\n",
      "✅ Augmented data for #text_000193.txt created successfully.\n",
      "✅ Augmented data for #text_000194.txt created successfully.\n",
      "✅ Augmented data for #text_000195.txt created successfully.\n",
      "✅ Augmented data for #text_000196.txt created successfully.\n",
      "✅ Augmented data for #text_000197.txt created successfully.\n",
      "✅ Augmented data for #text_000198.txt created successfully.\n",
      "✅ Augmented data for #text_000199.txt created successfully.\n",
      "✅ Augmented data for #text_000200.txt created successfully.\n",
      "✅ Augmented data for #text_000201.txt created successfully.\n",
      "✅ Augmented data for #text_000202.txt created successfully.\n",
      "✅ Augmented data for #text_000203.txt created successfully.\n",
      "✅ Augmented data for #text_000204.txt created successfully.\n",
      "✅ Augmented data for #text_000205.txt created successfully.\n",
      "✅ Augmented data for #text_000206.txt created successfully.\n",
      "✅ Augmented data for #text_000207.txt created successfully.\n",
      "✅ Augmented data for #text_000208.txt created successfully.\n",
      "✅ Augmented data for #text_000209.txt created successfully.\n",
      "✅ Augmented data for #text_000210.txt created successfully.\n",
      "✅ Augmented data for #text_000211.txt created successfully.\n",
      "✅ Augmented data for #text_000212.txt created successfully.\n",
      "✅ Augmented data for #text_000213.txt created successfully.\n",
      "✅ Augmented data for #text_000214.txt created successfully.\n",
      "✅ Augmented data for #text_000215.txt created successfully.\n",
      "✅ Augmented data for #text_000216.txt created successfully.\n",
      "✅ Augmented data for #text_000217.txt created successfully.\n",
      "✅ Augmented data for #text_000218.txt created successfully.\n",
      "✅ Augmented data for #text_000219.txt created successfully.\n",
      "✅ Augmented data for #text_000220.txt created successfully.\n",
      "✅ Augmented data for #text_000221.txt created successfully.\n",
      "✅ Augmented data for #text_000222.txt created successfully.\n",
      "✅ Augmented data for #text_000223.txt created successfully.\n",
      "✅ Augmented data for #text_000224.txt created successfully.\n",
      "✅ Augmented data for #text_000225.txt created successfully.\n",
      "✅ Augmented data for #text_000226.txt created successfully.\n",
      "✅ Augmented data for #text_000227.txt created successfully.\n",
      "✅ Augmented data for #text_000228.txt created successfully.\n",
      "✅ Augmented data for #text_000229.txt created successfully.\n",
      "✅ Augmented data for #text_000230.txt created successfully.\n",
      "✅ Augmented data for #text_000231.txt created successfully.\n",
      "✅ Augmented data for #text_000232.txt created successfully.\n",
      "✅ Augmented data for #text_000233.txt created successfully.\n",
      "✅ Augmented data for #text_000234.txt created successfully.\n",
      "✅ Augmented data for #text_000235.txt created successfully.\n",
      "✅ Augmented data for #text_000236.txt created successfully.\n",
      "✅ Augmented data for #text_000237.txt created successfully.\n",
      "✅ Augmented data for #text_000238.txt created successfully.\n",
      "✅ Augmented data for #text_000239.txt created successfully.\n",
      "✅ Augmented data for #text_000240.txt created successfully.\n",
      "✅ Augmented data for #text_000241.txt created successfully.\n",
      "✅ Augmented data for #text_000242.txt created successfully.\n",
      "✅ Augmented data for #text_000243.txt created successfully.\n",
      "✅ Augmented data for #text_000244.txt created successfully.\n",
      "✅ Augmented data for #text_000245.txt created successfully.\n",
      "✅ Augmented data for #text_000246.txt created successfully.\n",
      "✅ Augmented data for #text_000247.txt created successfully.\n",
      "✅ Augmented data for #text_000248.txt created successfully.\n",
      "✅ Augmented data for #text_000249.txt created successfully.\n",
      "✅ Augmented data for #text_000250.txt created successfully.\n",
      "✅ Augmented data for #text_000251.txt created successfully.\n",
      "✅ Augmented data for #text_000252.txt created successfully.\n",
      "✅ Augmented data for #text_000253.txt created successfully.\n",
      "✅ Augmented data for #text_000254.txt created successfully.\n",
      "✅ Augmented data for #text_000255.txt created successfully.\n",
      "✅ Augmented data for #text_000256.txt created successfully.\n",
      "✅ Augmented data for #text_000257.txt created successfully.\n",
      "✅ Augmented data for #text_000258.txt created successfully.\n",
      "✅ Augmented data for #text_000259.txt created successfully.\n",
      "✅ Augmented data for #text_000260.txt created successfully.\n",
      "✅ Augmented data for #text_000261.txt created successfully.\n",
      "✅ Augmented data for #text_000262.txt created successfully.\n",
      "✅ Augmented data for #text_000263.txt created successfully.\n",
      "✅ Augmented data for #text_000264.txt created successfully.\n",
      "✅ Augmented data for #text_000265.txt created successfully.\n",
      "✅ Augmented data for #text_000266.txt created successfully.\n",
      "✅ Augmented data for #text_000267.txt created successfully.\n",
      "✅ Augmented data for #text_000268.txt created successfully.\n",
      "✅ Augmented data for #text_000269.txt created successfully.\n",
      "✅ Augmented data for #text_000270.txt created successfully.\n",
      "✅ Augmented data for #text_000271.txt created successfully.\n",
      "✅ Augmented data for #text_000272.txt created successfully.\n",
      "✅ Augmented data for #text_000273.txt created successfully.\n",
      "✅ Augmented data for #text_000274.txt created successfully.\n",
      "✅ Augmented data for #text_000275.txt created successfully.\n",
      "✅ Augmented data for #text_000276.txt created successfully.\n",
      "✅ Augmented data for #text_000277.txt created successfully.\n",
      "✅ Augmented data for #text_000278.txt created successfully.\n",
      "✅ Augmented data for #text_000279.txt created successfully.\n",
      "✅ Augmented data for #text_000280.txt created successfully.\n",
      "✅ Augmented data for #text_000281.txt created successfully.\n",
      "✅ Augmented data for #text_000282.txt created successfully.\n",
      "✅ Augmented data for #text_000283.txt created successfully.\n",
      "✅ Augmented data for #text_000284.txt created successfully.\n",
      "✅ Augmented data for #text_000285.txt created successfully.\n",
      "✅ Augmented data for #text_000286.txt created successfully.\n",
      "✅ Augmented data for #text_000287.txt created successfully.\n",
      "✅ Augmented data for #text_000288.txt created successfully.\n",
      "✅ Augmented data for #text_000289.txt created successfully.\n",
      "✅ Augmented data for #text_000290.txt created successfully.\n",
      "✅ Augmented data for #text_000291.txt created successfully.\n",
      "✅ Augmented data for #text_000292.txt created successfully.\n",
      "✅ Augmented data for #text_000293.txt created successfully.\n",
      "✅ Augmented data for #text_000294.txt created successfully.\n",
      "✅ Augmented data for #text_000295.txt created successfully.\n",
      "✅ Augmented data for #text_000296.txt created successfully.\n",
      "✅ Augmented data for #text_000297.txt created successfully.\n",
      "✅ Augmented data for #text_000298.txt created successfully.\n",
      "✅ Augmented data for #text_000299.txt created successfully.\n",
      "✅ Augmented data for #text_000300.txt created successfully.\n",
      "✅ Augmented data for #text_000301.txt created successfully.\n",
      "✅ Augmented data for #text_000302.txt created successfully.\n",
      "✅ Augmented data for #text_000303.txt created successfully.\n",
      "✅ Augmented data for #text_000304.txt created successfully.\n",
      "✅ Augmented data for #text_000305.txt created successfully.\n",
      "✅ Augmented data for #text_000306.txt created successfully.\n",
      "✅ Augmented data for #text_000307.txt created successfully.\n",
      "✅ Augmented data for #text_000308.txt created successfully.\n",
      "✅ Augmented data for #text_000309.txt created successfully.\n",
      "✅ Augmented data for #text_000310.txt created successfully.\n",
      "✅ Augmented data for #text_000311.txt created successfully.\n",
      "✅ Augmented data for #text_000312.txt created successfully.\n",
      "✅ Augmented data for #text_000313.txt created successfully.\n",
      "✅ Augmented data for #text_000314.txt created successfully.\n",
      "✅ Augmented data for #text_000315.txt created successfully.\n",
      "✅ Augmented data for #text_000316.txt created successfully.\n",
      "✅ Augmented data for #text_000317.txt created successfully.\n",
      "✅ Augmented data for #text_000318.txt created successfully.\n",
      "✅ Augmented data for #text_000319.txt created successfully.\n",
      "✅ Augmented data for #text_000320.txt created successfully.\n",
      "✅ Augmented data for #text_000321.txt created successfully.\n",
      "✅ Augmented data for #text_000322.txt created successfully.\n",
      "✅ Augmented data for #text_000323.txt created successfully.\n",
      "✅ Augmented data for #text_000324.txt created successfully.\n",
      "✅ Augmented data for #text_000325.txt created successfully.\n",
      "✅ Augmented data for #text_000326.txt created successfully.\n",
      "✅ Augmented data for #text_000327.txt created successfully.\n",
      "✅ Augmented data for #text_000328.txt created successfully.\n",
      "✅ Augmented data for #text_000329.txt created successfully.\n",
      "✅ Augmented data for #text_000330.txt created successfully.\n",
      "✅ Augmented data for #text_000331.txt created successfully.\n",
      "✅ Augmented data for #text_000332.txt created successfully.\n",
      "✅ Augmented data for #text_000333.txt created successfully.\n",
      "✅ Augmented data for #text_000334.txt created successfully.\n",
      "✅ Augmented data for #text_000335.txt created successfully.\n",
      "✅ Augmented data for #text_000336.txt created successfully.\n",
      "✅ Augmented data for #text_000337.txt created successfully.\n",
      "✅ Augmented data for #text_000338.txt created successfully.\n",
      "✅ Augmented data for #text_000339.txt created successfully.\n",
      "✅ Augmented data for #text_000340.txt created successfully.\n",
      "✅ Augmented data for #text_000341.txt created successfully.\n",
      "✅ Augmented data for #text_000342.txt created successfully.\n",
      "✅ Augmented data for #text_000343.txt created successfully.\n",
      "✅ Augmented data for #text_000344.txt created successfully.\n",
      "✅ Augmented data for #text_000345.txt created successfully.\n",
      "✅ Augmented data for #text_000346.txt created successfully.\n",
      "✅ Augmented data for #text_000347.txt created successfully.\n",
      "✅ Augmented data for #text_000348.txt created successfully.\n",
      "✅ Augmented data for #text_000349.txt created successfully.\n",
      "✅ Augmented data for #text_000350.txt created successfully.\n",
      "✅ Augmented data for #text_000351.txt created successfully.\n",
      "✅ Augmented data for #text_000352.txt created successfully.\n",
      "✅ Augmented data for #text_000353.txt created successfully.\n",
      "✅ Augmented data for #text_000354.txt created successfully.\n",
      "✅ Augmented data for #text_000355.txt created successfully.\n",
      "✅ Augmented data for #text_000356.txt created successfully.\n",
      "✅ Augmented data for #text_000357.txt created successfully.\n",
      "✅ Augmented data for #text_000358.txt created successfully.\n",
      "✅ Augmented data for #text_000359.txt created successfully.\n",
      "✅ Augmented data for #text_000360.txt created successfully.\n",
      "✅ Augmented data for #text_000361.txt created successfully.\n",
      "✅ Augmented data for #text_000362.txt created successfully.\n",
      "✅ Augmented data for #text_000363.txt created successfully.\n",
      "✅ Augmented data for #text_000364.txt created successfully.\n",
      "✅ Augmented data for #text_000365.txt created successfully.\n",
      "✅ Augmented data for #text_000366.txt created successfully.\n",
      "✅ Augmented data for #text_000367.txt created successfully.\n",
      "✅ Augmented data for #text_000368.txt created successfully.\n",
      "✅ Augmented data for #text_000369.txt created successfully.\n",
      "✅ Augmented data for #text_000370.txt created successfully.\n",
      "✅ Augmented data for #text_000371.txt created successfully.\n",
      "✅ Augmented data for #text_000372.txt created successfully.\n",
      "✅ Augmented data for #text_000373.txt created successfully.\n",
      "✅ Augmented data for #text_000374.txt created successfully.\n",
      "✅ Augmented data for #text_000375.txt created successfully.\n",
      "✅ Augmented data for #text_000376.txt created successfully.\n",
      "✅ Augmented data for #text_000377.txt created successfully.\n",
      "✅ Augmented data for #text_000378.txt created successfully.\n",
      "✅ Augmented data for #text_000379.txt created successfully.\n",
      "✅ Augmented data for #text_000380.txt created successfully.\n",
      "✅ Augmented data for #text_000381.txt created successfully.\n",
      "✅ Augmented data for #text_000382.txt created successfully.\n",
      "✅ Augmented data for #text_000383.txt created successfully.\n",
      "✅ Augmented data for #text_000384.txt created successfully.\n",
      "✅ Augmented data for #text_000385.txt created successfully.\n",
      "✅ Augmented data for #text_000386.txt created successfully.\n",
      "✅ Augmented data for #text_000387.txt created successfully.\n",
      "✅ Augmented data for #text_000388.txt created successfully.\n",
      "✅ Augmented data for #text_000389.txt created successfully.\n",
      "✅ Augmented data for #text_000390.txt created successfully.\n",
      "✅ Augmented data for #text_000391.txt created successfully.\n",
      "✅ Augmented data for #text_000392.txt created successfully.\n",
      "✅ Augmented data for #text_000393.txt created successfully.\n",
      "✅ Augmented data for #text_000394.txt created successfully.\n",
      "✅ Augmented data for #text_000395.txt created successfully.\n",
      "✅ Augmented data for #text_000396.txt created successfully.\n",
      "✅ Augmented data for #text_000397.txt created successfully.\n",
      "✅ Augmented data for #text_000398.txt created successfully.\n",
      "✅ Augmented data for #text_000399.txt created successfully.\n",
      "✅ Augmented data for #text_000400.txt created successfully.\n",
      "✅ Augmented data for #text_000401.txt created successfully.\n",
      "✅ Augmented data for #text_000402.txt created successfully.\n",
      "✅ Augmented data for #text_000403.txt created successfully.\n",
      "✅ Augmented data for #text_000404.txt created successfully.\n",
      "✅ Augmented data for #text_000405.txt created successfully.\n",
      "✅ Augmented data for #text_000406.txt created successfully.\n",
      "✅ Augmented data for #text_000407.txt created successfully.\n",
      "✅ Augmented data for #text_000408.txt created successfully.\n",
      "✅ Augmented data for #text_000409.txt created successfully.\n",
      "✅ Augmented data for #text_000410.txt created successfully.\n",
      "✅ Augmented data for #text_000411.txt created successfully.\n",
      "✅ Augmented data for #text_000412.txt created successfully.\n",
      "✅ Augmented data for #text_000413.txt created successfully.\n",
      "✅ Augmented data for #text_000414.txt created successfully.\n",
      "✅ Augmented data for #text_000415.txt created successfully.\n",
      "✅ Augmented data for #text_000416.txt created successfully.\n",
      "✅ Augmented data for #text_000417.txt created successfully.\n",
      "✅ Augmented data for #text_000418.txt created successfully.\n",
      "✅ Augmented data for #text_000419.txt created successfully.\n",
      "✅ Augmented data for #text_000420.txt created successfully.\n",
      "✅ Augmented data for #text_000421.txt created successfully.\n",
      "✅ Augmented data for #text_000422.txt created successfully.\n",
      "✅ Augmented data for #text_000423.txt created successfully.\n",
      "✅ Augmented data for #text_000424.txt created successfully.\n",
      "✅ Augmented data for #text_000425.txt created successfully.\n",
      "✅ Augmented data for #text_000426.txt created successfully.\n",
      "✅ Augmented data for #text_000427.txt created successfully.\n",
      "✅ Augmented data for #text_000428.txt created successfully.\n",
      "✅ Augmented data for #text_000429.txt created successfully.\n",
      "✅ Augmented data for #text_000430.txt created successfully.\n",
      "✅ Augmented data for #text_000431.txt created successfully.\n",
      "✅ Augmented data for #text_000432.txt created successfully.\n",
      "✅ Augmented data for #text_000433.txt created successfully.\n",
      "✅ Augmented data for #text_000434.txt created successfully.\n",
      "✅ Augmented data for #text_000435.txt created successfully.\n",
      "✅ Augmented data for #text_000436.txt created successfully.\n",
      "✅ Augmented data for #text_000437.txt created successfully.\n",
      "✅ Augmented data for #text_000438.txt created successfully.\n",
      "✅ Augmented data for #text_000439.txt created successfully.\n",
      "✅ Augmented data for #text_000440.txt created successfully.\n",
      "✅ Augmented data for #text_000441.txt created successfully.\n",
      "✅ Augmented data for #text_000442.txt created successfully.\n",
      "✅ Augmented data for #text_000443.txt created successfully.\n",
      "✅ Augmented data for #text_000444.txt created successfully.\n",
      "✅ Augmented data for #text_000445.txt created successfully.\n",
      "✅ Augmented data for #text_000446.txt created successfully.\n",
      "✅ Augmented data for #text_000447.txt created successfully.\n",
      "✅ Augmented data for #text_000448.txt created successfully.\n",
      "✅ Augmented data for #text_000449.txt created successfully.\n",
      "✅ Augmented data for #text_000450.txt created successfully.\n",
      "✅ Augmented data for #text_000451.txt created successfully.\n",
      "✅ Augmented data for #text_000452.txt created successfully.\n",
      "✅ Augmented data for #text_000453.txt created successfully.\n",
      "✅ Augmented data for #text_000454.txt created successfully.\n",
      "✅ Augmented data for #text_000455.txt created successfully.\n",
      "✅ Augmented data for #text_000456.txt created successfully.\n",
      "✅ Augmented data for #text_000457.txt created successfully.\n",
      "✅ Augmented data for #text_000458.txt created successfully.\n",
      "✅ Augmented data for #text_000459.txt created successfully.\n",
      "✅ Augmented data for #text_000460.txt created successfully.\n",
      "✅ Augmented data for #text_000461.txt created successfully.\n",
      "✅ Augmented data for #text_000462.txt created successfully.\n",
      "✅ Augmented data for #text_000463.txt created successfully.\n",
      "✅ Augmented data for #text_000464.txt created successfully.\n",
      "✅ Augmented data for #text_000465.txt created successfully.\n",
      "✅ Augmented data for #text_000466.txt created successfully.\n",
      "✅ Augmented data for #text_000467.txt created successfully.\n",
      "✅ Augmented data for #text_000468.txt created successfully.\n",
      "✅ Augmented data for #text_000469.txt created successfully.\n",
      "✅ Augmented data for #text_000470.txt created successfully.\n",
      "✅ Augmented data for #text_000471.txt created successfully.\n",
      "✅ Augmented data for #text_000472.txt created successfully.\n",
      "✅ Augmented data for #text_000473.txt created successfully.\n",
      "✅ Augmented data for #text_000474.txt created successfully.\n",
      "✅ Augmented data for #text_000475.txt created successfully.\n",
      "✅ Augmented data for #text_000476.txt created successfully.\n",
      "✅ Augmented data for #text_000477.txt created successfully.\n",
      "✅ Augmented data for #text_000478.txt created successfully.\n",
      "✅ Augmented data for #text_000479.txt created successfully.\n",
      "✅ Augmented data for #text_000480.txt created successfully.\n",
      "✅ Augmented data for #text_000481.txt created successfully.\n",
      "✅ Augmented data for #text_000482.txt created successfully.\n",
      "✅ Augmented data for #text_000483.txt created successfully.\n",
      "✅ Augmented data for #text_000484.txt created successfully.\n",
      "✅ Augmented data for #text_000485.txt created successfully.\n",
      "✅ Augmented data for #text_000486.txt created successfully.\n",
      "✅ Augmented data for #text_000487.txt created successfully.\n",
      "✅ Augmented data for #text_000488.txt created successfully.\n",
      "✅ Augmented data for #text_000489.txt created successfully.\n",
      "✅ Augmented data for #text_000490.txt created successfully.\n",
      "✅ Augmented data for #text_000491.txt created successfully.\n",
      "✅ Augmented data for #text_000492.txt created successfully.\n",
      "✅ Augmented data for #text_000493.txt created successfully.\n",
      "✅ Augmented data for #text_000494.txt created successfully.\n",
      "✅ Augmented data for #text_000495.txt created successfully.\n",
      "✅ All augmented text data have been successfully uploaded.\n"
     ]
    }
   ],
   "source": [
    "# Create augmented text data\n",
    "text_augmentation(src_bucket = \"training-data-construction-zone\", dest_bucket = \"training-data-construction-zone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
