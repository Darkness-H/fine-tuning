{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cefc238-8601-4418-a77c-10c2b7c198a3",
   "metadata": {},
   "source": [
    "## ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81cc0dc-4227-44a0-81f1-5944c47ca52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:10.758994Z",
     "start_time": "2025-10-27T21:22:10.754679Z"
    },
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import torch\n",
    "import boto3\n",
    "import random\n",
    "import chromadb\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 10721\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6387f5-9efc-4d28-9dd7-543c425afbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c4c9b8-7890-41fb-b2c3-b68b47378ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP ViT-L/16\n",
    "model_name = \"ViT-B-16\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d5b8c4-eee2-47b5-a9c6-bb84051af30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CLIP-ViT-B-16\n",
      "Total parameters: 149,620,737\n",
      "Trainable parameters: 149,620,737\n",
      "The parameters are in: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# ---- Show parameter counts ----\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: CLIP-ViT-B-16\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"The parameters are in: {str(next(model.parameters()).dtype)}\") # FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad48e98-d882-4d9d-a6a0-962ad0a1b109",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743c2079-58a7-46c5-892e-9745549e28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import functools\n",
    "from torchvision import transforms\n",
    "from typing import List, Tuple\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data_bucket, data_prefix, s3):\n",
    "        self.data_bucket = data_bucket\n",
    "        self.data_prefix = data_prefix\n",
    "        self.s3 = s3\n",
    "\n",
    "        # Data keys\n",
    "        self.image_keys, self.text_keys = self.__loadfromminio__(data_bucket, data_prefix)\n",
    "\n",
    "    def __loadfromminio__(self, data_bucket, data_prefix):\n",
    "        image_keys = []\n",
    "        text_keys = []\n",
    "        paginator = self.s3.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=data_bucket, Prefix=data_prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if obj['Size'] == 0 and key.endswith(\"/\"):\n",
    "                    continue\n",
    "                if \"image\" in key.split(\"/\")[1]: # We only need images to find their corresponding description in MinIO\n",
    "                    image_keys.append(key)\n",
    "                    text_key = data_prefix + key.split(\"/\")[1].replace(\"image\", \"text\").replace(\"png\", \"txt\")\n",
    "                    text_keys.append(text_key)\n",
    "\n",
    "        # From lists to arrays\n",
    "        image_keys = np.array(image_keys)\n",
    "        text_keys = np.array(text_keys)\n",
    "        \n",
    "        return image_keys, text_keys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_keys)\n",
    "\n",
    "    def __getfile__(self, data_bucket, key, filetype = \"image\"):\n",
    "        resp = self.s3.get_object(Bucket=data_bucket, Key=key)\n",
    "        body = resp[\"Body\"].read()\n",
    "        if filetype == \"image\":\n",
    "            file = Image.open(BytesIO(body))\n",
    "        else: # filetype = \"text\"\n",
    "            file = body.decode(\"utf-8\")\n",
    "        return file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load image\n",
    "        image = self.__getfile__(self.data_bucket, self.image_keys[idx], filetype = \"image\")\n",
    "        \n",
    "        # Load text\n",
    "        text = self.__getfile__(self.data_bucket, self.text_keys[idx], filetype = \"text\")\n",
    "\n",
    "        return image, text\n",
    "\n",
    "# Collate function that applies preprocess and tokenizer to the batch\n",
    "def collate_fn(batch: List[Tuple[\"PIL.Image.Image\", str]], preprocess, tokenizer, pad_value: int = 0):\n",
    "    \"\"\"\n",
    "    batch: list of (PIL.Image, text_str)\n",
    "    preprocess: image preprocessing transform (from open_clip.create_model_and_transforms)\n",
    "    tokenizer: open_clip tokenizer callable\n",
    "    pad_value: value used to pad token sequences (default 0)\n",
    "\n",
    "    Returns:\n",
    "        images: torch.Tensor [B, C, H, W]\n",
    "        text_tokens: torch.LongTensor [B, L]\n",
    "        raw_texts: list[str]\n",
    "    \"\"\"\n",
    "    \n",
    "    images_pil, raw_texts = zip(*batch) # tuples\n",
    "\n",
    "    # --- Images: apply model-specific preprocess (PIL->Tensor) and stack ---\n",
    "    images = [preprocess(img) for img in images_pil] # each should be a Tensor\n",
    "    images = torch.stack(images, dim=0) # [B, C, H, W]\n",
    "\n",
    "    # --- Texts: use tokenizer ---\n",
    "    # Many open_clip tokenizers accept a list[str] and return a torch.LongTensor [B, L].\n",
    "    # But sometimes they may return a list of tensors or lists. Handle both cases.\n",
    "    tokenized = tokenizer(raw_texts) # [B, L]\n",
    "\n",
    "    return images, tokenized\n",
    "\n",
    "# Wrap collate_fn so DataLoader only sees a single-argument function\n",
    "collate = functools.partial(collate_fn, preprocess=preprocess, tokenizer=tokenizer, pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cd9bd2-0882-4593-9de8-aae65d949e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customized dataset object\n",
    "baseline_dataset = ImageTextDataset(\n",
    "    data_bucket = \"training-data-construction-zone\",\n",
    "    data_prefix = \"baseline-training-data/\",\n",
    "    s3 = s3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85188741-0c13-4053-8465-ec2651bb4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a shuffle over the dataset to prevent the model from learning order-based patterns\n",
    "dataloader = DataLoader(baseline_dataset, batch_size=16, shuffle=True, collate_fn=collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c752e4-d951-4194-bdc2-54de17a5a210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\242368609.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
      "C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\242368609.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(): \u001b[38;5;66;03m# the autocast() context ensures operations automatically use FP16 where safe\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     image_features, text_features \u001b[38;5;241m=\u001b[39m model(images, texts)\n\u001b[0;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(image_features, text_features)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Test fine-tuning on ViT-B-16 with Mixed/Reduced Precision (FP16)\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
    "# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n",
    "# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n",
    "        image_features, text_features = model(images, texts)\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c234c0-3e7b-44d8-979e-6ffb9f8c7bb2",
   "metadata": {},
   "source": [
    "### Mixed/Reduced Precision (FP16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b36c70-14b5-4e04-9541-71854b305a31",
   "metadata": {},
   "source": [
    "Memory usage is roughly halved, and training is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a556b65-16dd-42a6-9434-a75462881703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4b2571-41e5-4617-bc73-2a7f4004ab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\2601931610.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
      "C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\2601931610.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(): \u001b[38;5;66;03m# the autocast() context ensures operations automatically use FP16 where safe\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     image_features, text_features \u001b[38;5;241m=\u001b[39m model(images, \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(image_features, text_features)\n\u001b[0;32m     19\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:256\u001b[0m, in \u001b[0;36mSimpleTokenizer.__call__\u001b[1;34m(self, texts, context_length)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# use reduction strategy for tokenize if set, otherwise default to truncation below\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn(\n\u001b[0;32m    249\u001b[0m         texts,\n\u001b[0;32m    250\u001b[0m         context_length\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         encode_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode,\n\u001b[0;32m    254\u001b[0m     )\n\u001b[1;32m--> 256\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_token_id] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meot_token_id] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:215\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    214\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 215\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    217\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:85\u001b[0m, in \u001b[0;36m_clean_lower\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_clean_lower\u001b[39m(x):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# basic, remove whitespace, lower case\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:67\u001b[0m, in \u001b[0;36mbasic_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbasic_clean\u001b[39m(text):\n\u001b[1;32m---> 67\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mftfy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\ftfy\\__init__.py:349\u001b[0m, in \u001b[0;36mfix_text\u001b[1;34m(text, config, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[1;32m--> 349\u001b[0m     textbreak \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m textbreak \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m         textbreak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
    "# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n",
    "# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n",
    "        image_features, text_features = model(images, tokenizer(texts))\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0004e7-4f07-4a50-9e37-59898b465839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half() # FP32 -> FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11093a0c-677b-4ad5-9dab-fb70bb48e5d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We must convert the input tensors?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m text_tokens\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "# We must convert the input tensors?\n",
    "image = image.half().to(device)\n",
    "text_tokens = text_tokens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea193656-26e6-4ba5-9270-3482dc1b00e0",
   "metadata": {},
   "source": [
    "### Quantization (INT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31439f7c-abbc-49d1-a3d8-05df7ecf84bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:397\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[1;32m----> 3\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-L-14\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_config\u001b[38;5;241m=\u001b[39mbnb_config)\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:510\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[1;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    508\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:568\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m ):\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:889\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:862\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:399\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = open_clip.create_model(\"ViT-L-14\", quantized=True, bnb_config=bnb_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae070a66-ab08-49d4-97df-c822128640db",
   "metadata": {},
   "source": [
    "### LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c312c62-04c6-42c9-aa40-91e75b6c2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the best option for fine-tuning CLIP:\n",
    "# * Freeze the whole model\n",
    "# * Insert small trainable LoRA layers\n",
    "# * Train only 1â€“2% new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121df46c-3edd-4fe9-b803-ff1c7e30745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a86bac9-b859-4cf8-9ec4-bff3f8db841a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIP' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[0;32m      3\u001b[0m lora_cfg \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m      4\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m      5\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      6\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      7\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# typical for transformer models\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m get_peft_model(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m, lora_cfg)\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CLIP' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for transformer models\n",
    ")\n",
    "\n",
    "model.text = get_peft_model(model.text, lora_cfg)\n",
    "model.text.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b761b122-a52a-41ee-b39f-6511957d11b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (patch_dropout): Identity()\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): ModuleList(\n",
      "        (0-11): 12 x ResidualAttentionBlock(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ls_1): Identity()\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): GELU(approximate='none')\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ls_2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0-11): 12 x ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "visual <class 'open_clip.transformer.VisionTransformer'>\n",
      "transformer <class 'open_clip.transformer.Transformer'>\n",
      "token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ln_final <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.ln_pre <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer <class 'open_clip.transformer.Transformer'>\n",
      "visual.transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "visual.transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.ln_post <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer <class 'open_clip.transformer.Transformer'>\n",
      "transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ln_final <class 'open_clip.transformer.LayerNorm'>\n"
     ]
    }
   ],
   "source": [
    "# print top-level modules\n",
    "print(model)\n",
    "\n",
    "# print children names\n",
    "for name, module in model.named_children():\n",
    "    print(name, type(module))\n",
    "\n",
    "# print a few text-related submodules (common in open_clip)\n",
    "for name, module in model.named_modules():\n",
    "    if \"token\" in name or \"transformer\" in name or \"ln_\" in name or \"text\" in name:\n",
    "        print(name, type(module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72ca34b6-57ed-40a6-9846-8509df6ab802",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_emb = model.token_embedding\n",
    "text_transformer = model.transformer\n",
    "ln_final = model.ln_final   # or model.ln_post depending on the printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5f8ae0-12f0-4a1f-bc8b-45b7fd8fb291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "# reeze the whole model (prepare for LoRA)\n",
    "#Usually you want to freeze the base model and train only small adapter parameters:\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable params:\", trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9357a9ef-9eb8-4315-952c-046492c5ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced resblocks.0.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.1.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.2.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.3.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.4.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.5.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.6.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.7.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.8.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.9.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.10.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Replaced resblocks.11.attn.out_proj with LoRA Linear (r=8, alpha=32)\n",
      "Trainable params: 98304 / 149719041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\2378032752.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, texts \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     74\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 75\u001b[0m     text_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:256\u001b[0m, in \u001b[0;36mSimpleTokenizer.__call__\u001b[1;34m(self, texts, context_length)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# use reduction strategy for tokenize if set, otherwise default to truncation below\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn(\n\u001b[0;32m    249\u001b[0m         texts,\n\u001b[0;32m    250\u001b[0m         context_length\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         encode_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode,\n\u001b[0;32m    254\u001b[0m     )\n\u001b[1;32m--> 256\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_token_id] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meot_token_id] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:215\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    214\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 215\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    217\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:85\u001b[0m, in \u001b[0;36m_clean_lower\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_clean_lower\u001b[39m(x):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# basic, remove whitespace, lower case\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:67\u001b[0m, in \u001b[0;36mbasic_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbasic_clean\u001b[39m(text):\n\u001b[1;32m---> 67\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mftfy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\ftfy\\__init__.py:349\u001b[0m, in \u001b[0;36mfix_text\u001b[1;34m(text, config, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[1;32m--> 349\u001b[0m     textbreak \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m textbreak \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m         textbreak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#Add LoRA adapters to transformer linear layers\n",
    "#pip install loralib\n",
    "\n",
    "# peft is primarily designed for Hugging Face models. For OpenCLIP, a simple and compatible approach is to use loralib (lightweight LoRA wrapper)\n",
    "\n",
    "### Wrap target Linear layers (q/k/v projections or all Linear)\n",
    "\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "\n",
    "# Example helper: wrap Linear modules within a module whose name contains 'transformer'\n",
    "def apply_lora_to_transformer(root_module, r=8, alpha=32, target_module_type=nn.Linear, name_filter=None):\n",
    "    \"\"\"\n",
    "    Wrap Linear layers inside root_module with LoRA.\n",
    "    - r, alpha: LoRA hyperparams\n",
    "    - name_filter: optional substring to filter which modules to wrap (e.g. 'attn' or 'q_proj')\n",
    "    \"\"\"\n",
    "    for name, mod in root_module.named_modules():\n",
    "        # We only want to wrap the *leaf* Linear modules, not the parent modules\n",
    "        if isinstance(mod, target_module_type):\n",
    "            if name_filter is None or name_filter in name:\n",
    "                parent_path = name.rsplit('.', 1)[0] if '.' in name else ''\n",
    "                # rebind module in parent\n",
    "                parent = root_module\n",
    "                if parent_path:\n",
    "                    for part in parent_path.split('.'):\n",
    "                        parent = getattr(parent, part)\n",
    "                attr_name = name.split('.')[-1]\n",
    "                orig = getattr(parent, attr_name)\n",
    "                # create LoRA-wrapped layer with same in/out dims\n",
    "                lora_layer = lora.Linear(orig.in_features, orig.out_features, r=r, lora_alpha=alpha, bias=(orig.bias is not None))\n",
    "                # copy weight and bias\n",
    "                lora_layer.weight.data = orig.weight.data.clone()\n",
    "                if orig.bias is not None:\n",
    "                    lora_layer.bias.data = orig.bias.data.clone()\n",
    "                # replace\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "                print(f\"Replaced {name} with LoRA Linear (r={r}, alpha={alpha})\")\n",
    "\n",
    "# Apply to the text transformer\n",
    "apply_lora_to_transformer(model.transformer, r=8, alpha=32, name_filter=\"attn\")  # focus on attention proj\n",
    "\n",
    "\n",
    "# name_filter helps target q_proj, k_proj, v_proj, or modules with attn in the path.\n",
    "#Inspect your printed module names to choose an appropriate filter.\n",
    "\n",
    "# This replaces nn.Linear objects with loralib.Linear that contain LoRA parameters;\n",
    "# those LoRA parameters will be trainable while base weights remain frozen (unless you unfreeze them).\n",
    "\n",
    "### Make LoRA params trainable and check\n",
    "\n",
    "# Ensure base params still frozen, LoRA params trainable\n",
    "for name, p in model.named_parameters():\n",
    "    if \"lora\" in name.lower() or \"lora\" in name:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Print trainable params count\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total}\")\n",
    "\n",
    "### Training loop: mixed precision + optimizer\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images = images.to(device)\n",
    "    text_tokens = tokenizer(texts).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        image_features = model.encode_image(images)   # or model.visual(images)\n",
    "        text_features = model.encode_text(text_tokens) # or model.transformer(...)\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291c277-d8ba-4075-9c98-c71878b79f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec9dbb-daf0-4ad0-a635-d418fbe390c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:09.711655Z",
     "start_time": "2025-10-27T21:22:09.695004Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b94b12-3dde-4ab1-bd66-364e4bfeef55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:53.421060Z",
     "start_time": "2025-10-27T21:22:52.802581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to the server (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Create or get the collection named \"texts_images\" to store embeddings of images and texts\n",
    "collection_texts_images = client.create_collection(name=\"texts_images\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6530a273-faa1-4600-94d3-f0e6f2fd172e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'training-data-construction-zone' already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1877A5207DB01D9E',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'AAAAAA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '1877A5207DB01D9E',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2107',\n",
       "   'x-ratelimit-remaining': '2107',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Thu, 13 Nov 2025 18:42:18 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ChecksumCRC32': 'AAAAAA==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new Bucket in Min-IO to store our training data\n",
    "\n",
    "# List existing buckets\n",
    "buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n",
    "\n",
    "# Function that given a name, creates a bucket\n",
    "def createBucket(name, list_buckets):\n",
    "    if name in list_buckets:\n",
    "        print(f\"Bucket '{name}' already exists!\")\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=name)\n",
    "        print(f\"Created bucket: {name}\")\n",
    "\n",
    "# Create a bucket named landing_zone\n",
    "createBucket(\"training-data-construction-zone\", buckets)\n",
    "# Sub-bucket: Baseline Training Data\n",
    "s3.put_object(Bucket=\"training-data-construction-zone\", Key=\"baseline-training-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb0a71f-ce37-46a4-a82a-e4561e96e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
    "tokenizer = open_clip.get_tokenizer(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\") # Tokenizer for texts\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ec0fb2f-fc84-4300-910d-8ea911ed1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "# We can use this function to retrieve an text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text\n",
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "313a798d-b5cb-4764-b141-647bf5ba6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a similarity search for each text description in the dataset \n",
    "# to retrieve the most similar image, forming imageâ€“text pairs for training.\n",
    "def baseline_training_data_generator(src_bucket, dest_bucket, collection, model_text, tokenizer, src_prefix=\"texts/\", dest_prefix=\"baseline-training-data/\"):\n",
    "\n",
    "    # Incremental id assigned to each image-text pair\n",
    "    id_counter = 0\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=src_prefix):\n",
    "\n",
    "        # List of paths (meta_data)\n",
    "        image_paths = []\n",
    "        # List of embeddings\n",
    "        embeddings = []\n",
    "        # List of unique IDs for each embedding\n",
    "        ids = []\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "            # Get the description\n",
    "            description = get_text(src_bucket, key)\n",
    "            # Get the embeddings of the description\n",
    "            q_vec = embed_text(model_text, tokenizer, description)\n",
    "            # Apply the similarity search using the description\n",
    "            res_image = collection.query(\n",
    "                query_embeddings=[q_vec],\n",
    "                n_results=1,\n",
    "                where={\"type\": \"image\"}, # Filter by metadata type\n",
    "                include=[\"documents\", \"distances\"]\n",
    "            )\n",
    "            # Get the key for the image\n",
    "            key_image = res_image['documents'][0][0][len(src_bucket) + 1:]\n",
    "\n",
    "            # Remove the prefix part from the key\n",
    "            new_key_text = dest_prefix + \"text_\" + str(id_counter).zfill(6) + \".txt\" # ids of 000001, 000002, ...\n",
    "            new_key_image = dest_prefix + \"image_\" + str(id_counter).zfill(6) + \".png\" # ids of 000001, 000002, ...\n",
    "\n",
    "            # Copy objects without top-level folder and rename them\n",
    "            copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "            copy_source_image = {\"Bucket\": src_bucket, \"Key\": key_image}\n",
    "            s3.copy_object(Bucket=dest_bucket, Key=new_key_text, CopySource=copy_source_text)\n",
    "            s3.copy_object(Bucket=dest_bucket, Key=new_key_image, CopySource=copy_source_image)\n",
    "\n",
    "            print(f\"âœ… Baseline training pair #{id_counter} created successfully.\")\n",
    "\n",
    "    print(f\"âœ… All training pairs have been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ffa5b-6629-4ca1-ad0f-dd5dd9afa495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
