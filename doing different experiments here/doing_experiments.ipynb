{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cefc238-8601-4418-a77c-10c2b7c198a3",
   "metadata": {},
   "source": [
    "## ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81cc0dc-4227-44a0-81f1-5944c47ca52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:10.758994Z",
     "start_time": "2025-10-27T21:22:10.754679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import torch\n",
    "import boto3\n",
    "import random\n",
    "import chromadb\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 10721\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c4c9b8-7890-41fb-b2c3-b68b47378ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP ViT-L/16\n",
    "model_name = \"ViT-B-16\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82d5b8c4-eee2-47b5-a9c6-bb84051af30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CLIP-ViT-B-16\n",
      "Total parameters: 149,620,737\n",
      "Trainable parameters: 149,620,737\n",
      "The parameters are in: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# ---- Show parameter counts ----\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: CLIP-ViT-B-16\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"The parameters are in: {str(next(model.parameters()).dtype)}\") # FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c234c0-3e7b-44d8-979e-6ffb9f8c7bb2",
   "metadata": {},
   "source": [
    "### Mixed/Reduced Precision (FP16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b36c70-14b5-4e04-9541-71854b305a31",
   "metadata": {},
   "source": [
    "Memory usage is roughly halved, and training is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a556b65-16dd-42a6-9434-a75462881703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b2571-41e5-4617-bc73-2a7f4004ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
    "# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n",
    "# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n",
    "        image_features, text_features = model(images, tokenizer(texts))\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0004e7-4f07-4a50-9e37-59898b465839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half() # FP32 -> FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11093a0c-677b-4ad5-9dab-fb70bb48e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must convert the input tensors?\n",
    "image = image.half().to(device)\n",
    "text_tokens = text_tokens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea193656-26e6-4ba5-9270-3482dc1b00e0",
   "metadata": {},
   "source": [
    "### Quantization (INT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31439f7c-abbc-49d1-a3d8-05df7ecf84bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = open_clip.create_model(\"ViT-L-14\", quantized=True, bnb_config=bnb_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae070a66-ab08-49d4-97df-c822128640db",
   "metadata": {},
   "source": [
    "### LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c312c62-04c6-42c9-aa40-91e75b6c2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the best option for fine-tuning CLIP:\n",
    "# * Freeze the whole model\n",
    "# * Insert small trainable LoRA layers\n",
    "# * Train only 1â€“2% new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121df46c-3edd-4fe9-b803-ff1c7e30745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86bac9-b859-4cf8-9ec4-bff3f8db841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for transformer models\n",
    ")\n",
    "\n",
    "model.text = get_peft_model(model.text, lora_cfg)\n",
    "model.text.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b761b122-a52a-41ee-b39f-6511957d11b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (patch_dropout): Identity()\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): ModuleList(\n",
      "        (0-11): 12 x ResidualAttentionBlock(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ls_1): Identity()\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): GELU(approximate='none')\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ls_2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0-11): 12 x ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "visual <class 'open_clip.transformer.VisionTransformer'>\n",
      "transformer <class 'open_clip.transformer.Transformer'>\n",
      "token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ln_final <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.ln_pre <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer <class 'open_clip.transformer.Transformer'>\n",
      "visual.transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "visual.transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.ln_post <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer <class 'open_clip.transformer.Transformer'>\n",
      "transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ln_final <class 'open_clip.transformer.LayerNorm'>\n"
     ]
    }
   ],
   "source": [
    "# print top-level modules\n",
    "print(model)\n",
    "\n",
    "# print children names\n",
    "for name, module in model.named_children():\n",
    "    print(name, type(module))\n",
    "\n",
    "# print a few text-related submodules (common in open_clip)\n",
    "for name, module in model.named_modules():\n",
    "    if \"token\" in name or \"transformer\" in name or \"ln_\" in name or \"text\" in name:\n",
    "        print(name, type(module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72ca34b6-57ed-40a6-9846-8509df6ab802",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_emb = model.token_embedding\n",
    "text_transformer = model.transformer\n",
    "ln_final = model.ln_final   # or model.ln_post depending on the printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b5f8ae0-12f0-4a1f-bc8b-45b7fd8fb291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "# reeze the whole model (prepare for LoRA)\n",
    "#Usually you want to freeze the base model and train only small adapter parameters:\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable params:\", trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357a9ef-9eb8-4315-952c-046492c5ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add LoRA adapters to transformer linear layers\n",
    "#pip install loralib\n",
    "\n",
    "# peft is primarily designed for Hugging Face models. For OpenCLIP, a simple and compatible approach is to use loralib (lightweight LoRA wrapper)\n",
    "\n",
    "### Wrap target Linear layers (q/k/v projections or all Linear)\n",
    "\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "\n",
    "# Example helper: wrap Linear modules within a module whose name contains 'transformer'\n",
    "def apply_lora_to_transformer(root_module, r=8, alpha=32, target_module_type=nn.Linear, name_filter=None):\n",
    "    \"\"\"\n",
    "    Wrap Linear layers inside root_module with LoRA.\n",
    "    - r, alpha: LoRA hyperparams\n",
    "    - name_filter: optional substring to filter which modules to wrap (e.g. 'attn' or 'q_proj')\n",
    "    \"\"\"\n",
    "    for name, mod in root_module.named_modules():\n",
    "        # We only want to wrap the *leaf* Linear modules, not the parent modules\n",
    "        if isinstance(mod, target_module_type):\n",
    "            if name_filter is None or name_filter in name:\n",
    "                parent_path = name.rsplit('.', 1)[0] if '.' in name else ''\n",
    "                # rebind module in parent\n",
    "                parent = root_module\n",
    "                if parent_path:\n",
    "                    for part in parent_path.split('.'):\n",
    "                        parent = getattr(parent, part)\n",
    "                attr_name = name.split('.')[-1]\n",
    "                orig = getattr(parent, attr_name)\n",
    "                # create LoRA-wrapped layer with same in/out dims\n",
    "                lora_layer = lora.Linear(orig.in_features, orig.out_features, r=r, lora_alpha=alpha, bias=(orig.bias is not None))\n",
    "                # copy weight and bias\n",
    "                lora_layer.weight.data = orig.weight.data.clone()\n",
    "                if orig.bias is not None:\n",
    "                    lora_layer.bias.data = orig.bias.data.clone()\n",
    "                # replace\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "                print(f\"Replaced {name} with LoRA Linear (r={r}, alpha={alpha})\")\n",
    "\n",
    "# Apply to the text transformer\n",
    "apply_lora_to_transformer(model.transformer, r=8, alpha=32, name_filter=\"attn\")  # focus on attention proj\n",
    "\n",
    "\n",
    "# name_filter helps target q_proj, k_proj, v_proj, or modules with attn in the path.\n",
    "#Inspect your printed module names to choose an appropriate filter.\n",
    "\n",
    "# This replaces nn.Linear objects with loralib.Linear that contain LoRA parameters;\n",
    "# those LoRA parameters will be trainable while base weights remain frozen (unless you unfreeze them).\n",
    "\n",
    "### Make LoRA params trainable and check\n",
    "\n",
    "# Ensure base params still frozen, LoRA params trainable\n",
    "for name, p in model.named_parameters():\n",
    "    if \"lora\" in name.lower() or \"lora\" in name:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Print trainable params count\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total}\")\n",
    "\n",
    "### Training loop: mixed precision + optimizer\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images = images.to(device)\n",
    "    text_tokens = tokenizer(texts).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        image_features = model.encode_image(images)   # or model.visual(images)\n",
    "        text_features = model.encode_text(text_tokens) # or model.transformer(...)\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8f1b4-2d3e-4b8f-91c9-dc8032479197",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee6171-5b80-4565-8d63-08de9e3bca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_root, tokenizer, image_transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_root = image_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = f\"{self.image_root}/{self.df.iloc[idx]['image_path']}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        text_tokens = self.tokenizer([caption])[0]\n",
    "\n",
    "        return image, text_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97ff6c-4c13-4a27-9761-407bea6be5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageTextDataset(\n",
    "    csv_path=\"train.csv\",\n",
    "    image_root=\"images/\",\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291c277-d8ba-4075-9c98-c71878b79f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec9dbb-daf0-4ad0-a635-d418fbe390c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:09.711655Z",
     "start_time": "2025-10-27T21:22:09.695004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b94b12-3dde-4ab1-bd66-364e4bfeef55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:53.421060Z",
     "start_time": "2025-10-27T21:22:52.802581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to the server (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Create or get the collection named \"texts_images\" to store embeddings of images and texts\n",
    "collection_texts_images = client.create_collection(name=\"texts_images\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6530a273-faa1-4600-94d3-f0e6f2fd172e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'training-data-construction-zone' already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1877A5207DB01D9E',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'AAAAAA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '1877A5207DB01D9E',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2107',\n",
       "   'x-ratelimit-remaining': '2107',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Thu, 13 Nov 2025 18:42:18 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ChecksumCRC32': 'AAAAAA==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new Bucket in Min-IO to store our training data\n",
    "\n",
    "# List existing buckets\n",
    "buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n",
    "\n",
    "# Function that given a name, creates a bucket\n",
    "def createBucket(name, list_buckets):\n",
    "    if name in list_buckets:\n",
    "        print(f\"Bucket '{name}' already exists!\")\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=name)\n",
    "        print(f\"Created bucket: {name}\")\n",
    "\n",
    "# Create a bucket named landing_zone\n",
    "createBucket(\"training-data-construction-zone\", buckets)\n",
    "# Sub-bucket: Baseline Training Data\n",
    "s3.put_object(Bucket=\"training-data-construction-zone\", Key=\"baseline-training-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb0a71f-ce37-46a4-a82a-e4561e96e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
    "tokenizer = open_clip.get_tokenizer(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\") # Tokenizer for texts\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ec0fb2f-fc84-4300-910d-8ea911ed1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "# We can use this function to retrieve an text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text\n",
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "313a798d-b5cb-4764-b141-647bf5ba6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a similarity search for each text description in the dataset \n",
    "# to retrieve the most similar image, forming imageâ€“text pairs for training.\n",
    "def baseline_training_data_generator(src_bucket, dest_bucket, collection, model_text, tokenizer, src_prefix=\"texts/\", dest_prefix=\"baseline-training-data/\"):\n",
    "\n",
    "    # Incremental id assigned to each image-text pair\n",
    "    id_counter = 0\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=src_prefix):\n",
    "\n",
    "        # List of paths (meta_data)\n",
    "        image_paths = []\n",
    "        # List of embeddings\n",
    "        embeddings = []\n",
    "        # List of unique IDs for each embedding\n",
    "        ids = []\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "            # Get the description\n",
    "            description = get_text(src_bucket, key)\n",
    "            # Get the embeddings of the description\n",
    "            q_vec = embed_text(model_text, tokenizer, description)\n",
    "            # Apply the similarity search using the description\n",
    "            res_image = collection.query(\n",
    "                query_embeddings=[q_vec],\n",
    "                n_results=1,\n",
    "                where={\"type\": \"image\"}, # Filter by metadata type\n",
    "                include=[\"documents\", \"distances\"]\n",
    "            )\n",
    "            # Get the key for the image\n",
    "            key_image = res_image['documents'][0][0][len(src_bucket) + 1:]\n",
    "\n",
    "            # Remove the prefix part from the key\n",
    "            new_key_text = dest_prefix + \"text_\" + str(id_counter).zfill(6) + \".txt\" # ids of 000001, 000002, ...\n",
    "            new_key_image = dest_prefix + \"image_\" + str(id_counter).zfill(6) + \".png\" # ids of 000001, 000002, ...\n",
    "\n",
    "            # Copy objects without top-level folder and rename them\n",
    "            copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "            copy_source_image = {\"Bucket\": src_bucket, \"Key\": key_image}\n",
    "            s3.copy_object(Bucket=dest_bucket, Key=new_key_text, CopySource=copy_source_text)\n",
    "            s3.copy_object(Bucket=dest_bucket, Key=new_key_image, CopySource=copy_source_image)\n",
    "\n",
    "            print(f\"âœ… Baseline training pair #{id_counter} created successfully.\")\n",
    "\n",
    "    print(f\"âœ… All training pairs have been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ffa5b-6629-4ca1-ad0f-dd5dd9afa495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
