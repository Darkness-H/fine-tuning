{"cells":[{"cell_type":"markdown","id":"2cefc238-8601-4418-a77c-10c2b7c198a3","metadata":{"id":"2cefc238-8601-4418-a77c-10c2b7c198a3"},"source":["## ------------------------------"]},{"cell_type":"code","source":["!pip install boto3 chromadb open_clip_torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DwSLUhXL3S2","executionInfo":{"status":"ok","timestamp":1764086625430,"user_tz":-60,"elapsed":78702,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"96a274ab-28f1-4231-e2fa-8bd45a7e9941"},"id":"3DwSLUhXL3S2","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting boto3\n","  Downloading boto3-1.41.3-py3-none-any.whl.metadata (6.8 kB)\n","Collecting chromadb\n","  Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n","Collecting open_clip_torch\n","  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n","Collecting botocore<1.42.0,>=1.41.3 (from boto3)\n","  Downloading botocore-1.41.3-py3-none-any.whl.metadata (5.9 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.16.0,>=0.15.0 (from boto3)\n","  Downloading s3transfer-0.15.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting build>=1.0.3 (from chromadb)\n","  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n","Collecting pybase64>=1.4.1 (from chromadb)\n","  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n","Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n","Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n","  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.24.0+cu126)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2025.11.3)\n","Collecting ftfy (from open_clip_torch)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.7.0)\n","Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.22)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.3->boto3) (2.9.0.post0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.42.0,>=1.41.3->boto3) (2.5.0)\n","Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.42.0,>=1.41.3->boto3)\n","  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n","Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n","Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading boto3-1.41.3-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.41.3-py3-none-any.whl (14.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n","Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading s3transfer-0.15.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n","Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=80115aea5b4e9791526579aac6ca212ca5f00d255cdfa5338589483284b68c03\n","  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n","Successfully built pypika\n","Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, jmespath, humanfriendly, httptools, ftfy, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, botocore, s3transfer, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, boto3, opentelemetry-exporter-otlp-proto-grpc, open_clip_torch, chromadb\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.5.0\n","    Uninstalling urllib3-2.5.0:\n","      Successfully uninstalled urllib3-2.5.0\n","  Attempting uninstall: opentelemetry-proto\n","    Found existing installation: opentelemetry-proto 1.37.0\n","    Uninstalling opentelemetry-proto-1.37.0:\n","      Successfully uninstalled opentelemetry-proto-1.37.0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n","    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.37.0\n","    Uninstalling opentelemetry-api-1.37.0:\n","      Successfully uninstalled opentelemetry-api-1.37.0\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n","    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.37.0\n","    Uninstalling opentelemetry-sdk-1.37.0:\n","      Successfully uninstalled opentelemetry-sdk-1.37.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n","google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n","google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 boto3-1.41.3 botocore-1.41.3 build-1.3.0 chromadb-1.3.5 coloredlogs-15.0.1 durationpy-0.10 ftfy-6.3.1 httptools-0.7.1 humanfriendly-10.0 jmespath-1.0.1 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 open_clip_torch-3.2.0 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 s3transfer-0.15.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"]}]},{"cell_type":"code","execution_count":2,"id":"e81cc0dc-4227-44a0-81f1-5944c47ca52d","metadata":{"ExecuteTime":{"end_time":"2025-10-27T21:22:10.758994Z","start_time":"2025-10-27T21:22:10.754679Z"},"editable":true,"scrolled":true,"tags":[],"id":"e81cc0dc-4227-44a0-81f1-5944c47ca52d","executionInfo":{"status":"ok","timestamp":1764086703141,"user_tz":-60,"elapsed":77682,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}}},"outputs":[],"source":["# Importing useful dependencies\n","import io\n","import torch\n","import boto3\n","import random\n","import chromadb\n","import open_clip\n","import numpy as np\n","from PIL import Image\n","from io import BytesIO\n","from chromadb.config import Settings\n","\n","# Set a seed for reproducibility\n","SEED = 10721\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"code","execution_count":3,"id":"1f6387f5-9efc-4d28-9dd7-543c425afbce","metadata":{"id":"1f6387f5-9efc-4d28-9dd7-543c425afbce","executionInfo":{"status":"ok","timestamp":1764086704449,"user_tz":-60,"elapsed":1287,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}}},"outputs":[],"source":["# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n","s3 = boto3.client(\n","    \"s3\",\n","    #endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n","    endpoint_url=\"https://statueless-manducatory-renato.ngrok-free.dev\", # MinIO API endpoint (ngrok)\n","    aws_access_key_id=\"minioadmin\", # User name\n","    aws_secret_access_key=\"minioadmin\", # Password\n",")"]},{"cell_type":"code","execution_count":4,"id":"c6c4c9b8-7890-41fb-b2c3-b68b47378ee1","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["968b19cc6bff473399650af34399a861","10d8c1efdd734b46979695b4403f6531","f0401207902845fe94731ec4ea31f1a4","5602826c956f46109b394bccb0ba96a5","d87485205b5f4566a7b045ca2ca8c2eb","57011c8286484fd4b51bd40594dd4ac7","c04808686d6b49ef8ff355491e9d2446","e4946a822da74e67b2ec65cb9cf181cf","4c2698ed7db84e589fb5f3f6853cf2e3","f57afb85a7c04e3583639fb73e157a55","4c22da1870774f3bbafed1360b13fd2c"]},"id":"c6c4c9b8-7890-41fb-b2c3-b68b47378ee1","executionInfo":{"status":"ok","timestamp":1764086721195,"user_tz":-60,"elapsed":16716,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"93abf331-f152-4d02-b465-b1e088cef7e0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["open_clip_model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"968b19cc6bff473399650af34399a861"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["CLIP(\n","  (visual): VisionTransformer(\n","    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","    (patch_dropout): Identity()\n","    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (transformer): Transformer(\n","      (resblocks): ModuleList(\n","        (0-11): 12 x ResidualAttentionBlock(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ls_1): Identity()\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): GELU(approximate='none')\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ls_2): Identity()\n","        )\n","      )\n","    )\n","    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (resblocks): ModuleList(\n","      (0-11): 12 x ResidualAttentionBlock(\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ls_1): Identity()\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ls_2): Identity()\n","      )\n","    )\n","  )\n","  (token_embedding): Embedding(49408, 512)\n","  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":4}],"source":["# Just in case our device has gpu\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Load CLIP ViT-L/16\n","model_name = \"ViT-B-16\"\n","model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n","tokenizer = open_clip.get_tokenizer(model_name)\n","model.to(device)"]},{"cell_type":"code","execution_count":5,"id":"82d5b8c4-eee2-47b5-a9c6-bb84051af30b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82d5b8c4-eee2-47b5-a9c6-bb84051af30b","executionInfo":{"status":"ok","timestamp":1764086721262,"user_tz":-60,"elapsed":64,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"3ea135d1-fe15-4d49-ed51-c84b38e8c08d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: CLIP-ViT-B-16\n","Total parameters: 149,620,737\n","Trainable parameters: 149,620,737\n","The parameters are in: torch.float32\n"]}],"source":["# ---- Show parameter counts ----\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"Model: CLIP-ViT-B-16\")\n","print(f\"Total parameters: {total_params:,}\")\n","print(f\"Trainable parameters: {trainable_params:,}\")\n","print(f\"The parameters are in: {str(next(model.parameters()).dtype)}\") # FP32"]},{"cell_type":"markdown","id":"3ad48e98-d882-4d9d-a6a0-962ad0a1b109","metadata":{"id":"3ad48e98-d882-4d9d-a6a0-962ad0a1b109"},"source":["### Dataloader"]},{"cell_type":"code","execution_count":6,"id":"743c2079-58a7-46c5-892e-9745549e28f4","metadata":{"id":"743c2079-58a7-46c5-892e-9745549e28f4","executionInfo":{"status":"ok","timestamp":1764086721341,"user_tz":-60,"elapsed":62,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}}},"outputs":[],"source":["import pandas as pd\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import functools\n","from torchvision import transforms\n","from typing import List, Tuple\n","\n","class ImageTextDataset(Dataset):\n","    def __init__(self, data_bucket, data_prefix, s3):\n","        self.data_bucket = data_bucket\n","        self.data_prefix = data_prefix\n","        self.s3 = s3\n","\n","        # Data keys\n","        self.image_keys, self.text_keys = self.__loadfromminio__(data_bucket, data_prefix)\n","\n","    def __loadfromminio__(self, data_bucket, data_prefix):\n","        image_keys = []\n","        text_keys = []\n","        paginator = self.s3.get_paginator(\"list_objects_v2\")\n","        for page in paginator.paginate(Bucket=data_bucket, Prefix=data_prefix):\n","            for obj in page.get(\"Contents\", []):\n","                key = obj[\"Key\"]\n","                if obj['Size'] == 0 and key.endswith(\"/\"):\n","                    continue\n","                if \"image\" in key.split(\"/\")[1]: # We only need images to find their corresponding description in MinIO\n","                    image_keys.append(key)\n","                    text_key = data_prefix + key.split(\"/\")[1].replace(\"image\", \"text\").replace(\"png\", \"txt\")\n","                    text_keys.append(text_key)\n","\n","        # From lists to arrays\n","        image_keys = np.array(image_keys)\n","        text_keys = np.array(text_keys)\n","\n","        return image_keys, text_keys\n","\n","    def __len__(self):\n","        return len(self.image_keys)\n","\n","    def __getfile__(self, data_bucket, key, filetype = \"image\"):\n","        resp = self.s3.get_object(Bucket=data_bucket, Key=key)\n","        body = resp[\"Body\"].read()\n","        if filetype == \"image\":\n","            file = Image.open(BytesIO(body))\n","        else: # filetype = \"text\"\n","            file = body.decode(\"utf-8\")\n","        return file\n","\n","    def __getitem__(self, idx):\n","\n","        # Load image\n","        image = self.__getfile__(self.data_bucket, self.image_keys[idx], filetype = \"image\")\n","\n","        # Load text\n","        text = self.__getfile__(self.data_bucket, self.text_keys[idx], filetype = \"text\")\n","\n","        return image, text\n","\n","# Collate function that applies preprocess and tokenizer to the batch\n","def collate_fn(batch: List[Tuple[\"PIL.Image.Image\", str]], preprocess, tokenizer, pad_value: int = 0):\n","    \"\"\"\n","    batch: list of (PIL.Image, text_str)\n","    preprocess: image preprocessing transform (from open_clip.create_model_and_transforms)\n","    tokenizer: open_clip tokenizer callable\n","    pad_value: value used to pad token sequences (default 0)\n","\n","    Returns:\n","        images: torch.Tensor [B, C, H, W]\n","        text_tokens: torch.LongTensor [B, L]\n","        raw_texts: list[str]\n","    \"\"\"\n","\n","    images_pil, raw_texts = zip(*batch) # tuples\n","\n","    # --- Images: apply model-specific preprocess (PIL->Tensor) and stack ---\n","    images = [preprocess(img) for img in images_pil] # each should be a Tensor\n","    images = torch.stack(images, dim=0) # [B, C, H, W]\n","\n","    # --- Texts: use tokenizer ---\n","    # Many open_clip tokenizers accept a list[str] and return a torch.LongTensor [B, L].\n","    # But sometimes they may return a list of tensors or lists. Handle both cases.\n","    tokenized = tokenizer(raw_texts) # [B, L]\n","\n","    return images, tokenized\n","\n","# Wrap collate_fn so DataLoader only sees a single-argument function\n","collate = functools.partial(collate_fn, preprocess=preprocess, tokenizer=tokenizer, pad_value=0)"]},{"cell_type":"code","execution_count":7,"id":"94cd9bd2-0882-4593-9de8-aae65d949e1f","metadata":{"id":"94cd9bd2-0882-4593-9de8-aae65d949e1f","executionInfo":{"status":"ok","timestamp":1764086727292,"user_tz":-60,"elapsed":5924,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}}},"outputs":[],"source":["# Create customized dataset object\n","baseline_dataset = ImageTextDataset(\n","    data_bucket = \"training-data-construction-zone\",\n","    data_prefix = \"baseline-training-data/\",\n","    s3 = s3\n",")"]},{"cell_type":"code","execution_count":8,"id":"85188741-0c13-4053-8465-ec2651bb4eee","metadata":{"id":"85188741-0c13-4053-8465-ec2651bb4eee","executionInfo":{"status":"ok","timestamp":1764086727295,"user_tz":-60,"elapsed":1,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}}},"outputs":[],"source":["# Apply a shuffle over the dataset to prevent the model from learning order-based patterns\n","dataloader = DataLoader(baseline_dataset, batch_size=32, shuffle=True, collate_fn=collate, num_workers=0)\n","# Increasing the size of the batch slows down the training process, but generalizes better the model"]},{"cell_type":"code","source":["# Split the dataloader into train/dev/test"],"metadata":{"id":"72sxvMuNx7Ff"},"id":"72sxvMuNx7Ff","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"49c752e4-d951-4194-bdc2-54de17a5a210","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"49c752e4-d951-4194-bdc2-54de17a5a210","executionInfo":{"status":"error","timestamp":1764088715050,"user_tz":-60,"elapsed":1987754,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"729baf02-bb2d-4bbf-ff50-9e7baf1fe238"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3422335394.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n","/usr/local/lib/python3.12/dist-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  super().__init__(\n","/tmp/ipython-input-3422335394.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n","/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loss: 38.7926\n","Loss: 32.6842\n","Loss: 14.1557\n","Loss: 84.0698\n","Loss: 14.9327\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3422335394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.optim as optim\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import DataLoader, Dataset\n","from open_clip.loss import ClipLoss\n","import matplotlib.pyplot as plt\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.optim as optim\n","\n","criterion = ClipLoss()\n","\n","# Test fine-tuning on ViT-B-16 with Mixed/Reduced Precision (FP16)\n","\n","losses = []  # Store losses\n","\n","# AdamW optimizer\n","optimizer = optim.AdamW(model.parameters(), lr=1e-6) # Big lr leads to overfitting on the training data -> the model will get better results on the training data, but poor results on the test set\n","scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n","# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n","# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n","\n","for images, texts in dataloader:\n","    images, texts = images.to(device), texts.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n","        img_feats = model.encode_image(images)\n","        txt_feats = model.encode_text(texts)\n","        loss = criterion(img_feats, txt_feats, model.logit_scale.exp())\n","    print(f\"Loss: {loss.item():.4f}\")\n","\n","    # Save loss\n","    losses.append(loss.item())\n","\n","    scaler.scale(loss).backward()\n","    scaler.step(optimizer)\n","    scaler.update()\n","\n","# Plot the losses\n","plt.figure(figsize=(8,5))\n","plt.plot(losses, marker='o')\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Loss\")\n","plt.grid(True)\n","plt.show()\n","\n","# ~45 mins"]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"clip_finetuned.pt\")"],"metadata":{"id":"CVGsf-GZsWnl","executionInfo":{"status":"ok","timestamp":1764088727026,"user_tz":-60,"elapsed":6847,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}}},"id":"CVGsf-GZsWnl","execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Create model (openai weights)\n","model_base, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n","model_base.to(device)\n","\n","# Create model (uninitialized weights)\n","model_ft, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n","model_ft.to(device)\n","model_ft.load_state_dict(torch.load(\"clip_finetuned.pt\", map_location=device))\n","model_ft.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNOrYwXVtGuY","executionInfo":{"status":"ok","timestamp":1764088740433,"user_tz":-60,"elapsed":11007,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"1defc3a2-621e-4741-fc64-80b79d2c1bb6"},"id":"DNOrYwXVtGuY","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CLIP(\n","  (visual): VisionTransformer(\n","    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","    (patch_dropout): Identity()\n","    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (transformer): Transformer(\n","      (resblocks): ModuleList(\n","        (0-11): 12 x ResidualAttentionBlock(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ls_1): Identity()\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): GELU(approximate='none')\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ls_2): Identity()\n","        )\n","      )\n","    )\n","    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (resblocks): ModuleList(\n","      (0-11): 12 x ResidualAttentionBlock(\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ls_1): Identity()\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ls_2): Identity()\n","      )\n","    )\n","  )\n","  (token_embedding): Embedding(49408, 512)\n","  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","# Set both models to eval\n","model_ft.eval()    # fine-tuned\n","model_base.eval()  # original\n","\n","images, texts = next(iter(dataloader))  # take a batch\n","images, texts = images.to(device), texts.to(device)\n","\n","with torch.no_grad():\n","    # Base model\n","    img_feats_base = F.normalize(model_base.encode_image(images), dim=-1)\n","    txt_feats_base = F.normalize(model_base.encode_text(texts), dim=-1)\n","\n","    # Fine-tuned model\n","    img_feats_ft = F.normalize(model_ft.encode_image(images), dim=-1)\n","    txt_feats_ft = F.normalize(model_ft.encode_text(texts), dim=-1)\n","\n","# Compare embedding changes (cosine similarity)\n","cos_sim_img = (img_feats_base * img_feats_ft).sum(dim=-1).mean()\n","cos_sim_txt = (txt_feats_base * txt_feats_ft).sum(dim=-1).mean()\n","\n","print(f\"Avg cosine similarity for images: {cos_sim_img:.4f}\")\n","print(f\"Avg cosine similarity for text:   {cos_sim_txt:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MST26Ab0stDa","executionInfo":{"status":"ok","timestamp":1764088823340,"user_tz":-60,"elapsed":70714,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"e9c32cc4-4630-40fd-e7f9-d34c1ea57dd5"},"id":"MST26Ab0stDa","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Avg cosine similarity for images: 0.9989\n","Avg cosine similarity for text:   0.9991\n"]}]},{"cell_type":"markdown","source":["In short: negative cosine similarity to the original model is fine when fine-tuning for a task the base model didn’t see — the important thing is whether the fine-tuned model is performing well on your new objective.\n","\n","If you want, I can show a workflow to evaluate your fine-tuned model on the new task, so you can check performance without worrying about alignment to the original CLIP embeddings."],"metadata":{"id":"PMPmJCb2ur8h"},"id":"PMPmJCb2ur8h"},{"cell_type":"code","source":["def retrieval_accuracy(img_feats, txt_feats):\n","    # Compute similarity matrix\n","    sims = img_feats @ txt_feats.t()\n","    # Ground truth: assume diagonal matches\n","    labels = torch.arange(img_feats.size(0)).to(img_feats.device)\n","\n","    # Image->Text\n","    top1_i2t = (sims.argmax(dim=1) == labels).float().mean()\n","    # Text->Image\n","    top1_t2i = (sims.argmax(dim=0) == labels).float().mean()\n","\n","    return top1_i2t.item(), top1_t2i.item()\n","\n","acc_base = retrieval_accuracy(img_feats_base, txt_feats_base)\n","acc_ft   = retrieval_accuracy(img_feats_ft, txt_feats_ft)\n","\n","print(f\"Base model retrieval (image->text, text->image): {acc_base}\")\n","print(f\"Fine-tuned model retrieval: {acc_ft}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHM1iPGounTQ","executionInfo":{"status":"ok","timestamp":1764088831844,"user_tz":-60,"elapsed":39,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"03ca8727-50ee-4b55-ebe9-815304bc2af8"},"id":"iHM1iPGounTQ","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Base model retrieval (image->text, text->image): (0.84375, 0.84375)\n","Fine-tuned model retrieval: (0.84375, 0.875)\n"]}]},{"cell_type":"code","source":["from open_clip.loss import ClipLoss\n","\n","criterion = ClipLoss()\n","\n","loss_base = criterion(img_feats_base, txt_feats_base, model_base.logit_scale.exp())\n","loss_ft   = criterion(img_feats_ft, txt_feats_ft, model_ft.logit_scale.exp())\n","\n","print(f\"Base model contrastive loss: {loss_base.item():.4f}\")\n","print(f\"Fine-tuned model contrastive loss: {loss_ft.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPoz5Kr3u86-","executionInfo":{"status":"ok","timestamp":1764088900224,"user_tz":-60,"elapsed":46,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"0f329bbd-db13-4828-8c05-91e34e2de83c"},"id":"xPoz5Kr3u86-","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Base model contrastive loss: 0.4767\n","Fine-tuned model contrastive loss: 0.4814\n"]}]},{"cell_type":"code","source":["# similarity matrix\n","sims = img_feats_ft @ txt_feats_ft.t()\n","top_idx = sims[0].argmax()\n","print(f\"Most similar text to first image: {texts[top_idx]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ovF02gwvAQv","executionInfo":{"status":"ok","timestamp":1764088906562,"user_tz":-60,"elapsed":43,"user":{"displayName":"Geming Wu","userId":"11726503784716303318"}},"outputId":"f8b2b4c3-8e38-4bb4-f3f7-e3ad29ba5b88"},"id":"0ovF02gwvAQv","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Most similar text to first image: tensor([49406,   761,  1394,   274,   533,   550,  1488,   268,  1002,  1816,\n","         4377,   593,   518,  3638,  1378,   718,   525,  4511,   539,  7584,\n","          269,   585,   533,   518, 42582,  3644,   530,   518,   761,  1394,\n","         1857,  7479,   638, 25815,  6231,   267, 15361,  1241,   638,   518,\n","          874,  1237, 11442,  1110,  5149,   537,  1977,   518,   275,   640,\n","         1551,   783,  2296,  3410,   601,   902, 34774,   269, 11362, 14057,\n","          533,   518, 38183,   539,   518,  1063,   267,   822,   533, 34086,\n","         1265,  8201,   531,  2696,   593, 33940, 49407])\n"]}]},{"cell_type":"markdown","id":"09c234c0-3e7b-44d8-979e-6ffb9f8c7bb2","metadata":{"id":"09c234c0-3e7b-44d8-979e-6ffb9f8c7bb2"},"source":["### Mixed/Reduced Precision (FP16)"]},{"cell_type":"markdown","id":"45b36c70-14b5-4e04-9541-71854b305a31","metadata":{"id":"45b36c70-14b5-4e04-9541-71854b305a31"},"source":["Memory usage is roughly halved, and training is faster."]},{"cell_type":"code","execution_count":null,"id":"0a556b65-16dd-42a6-9434-a75462881703","metadata":{"id":"0a556b65-16dd-42a6-9434-a75462881703"},"outputs":[],"source":["# Two options:"]},{"cell_type":"code","execution_count":null,"id":"3e4b2571-41e5-4617-bc73-2a7f4004ab7a","metadata":{"id":"3e4b2571-41e5-4617-bc73-2a7f4004ab7a","outputId":"f1576821-bb7e-4702-9e07-4825858b7dcc"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\2601931610.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n","C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\2601931610.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n"]},{"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'find'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(): \u001b[38;5;66;03m# the autocast() context ensures operations automatically use FP16 where safe\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     image_features, text_features \u001b[38;5;241m=\u001b[39m model(images, \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(image_features, text_features)\n\u001b[0;32m     19\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:256\u001b[0m, in \u001b[0;36mSimpleTokenizer.__call__\u001b[1;34m(self, texts, context_length)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# use reduction strategy for tokenize if set, otherwise default to truncation below\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn(\n\u001b[0;32m    249\u001b[0m         texts,\n\u001b[0;32m    250\u001b[0m         context_length\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         encode_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode,\n\u001b[0;32m    254\u001b[0m     )\n\u001b[1;32m--> 256\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_token_id] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meot_token_id] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:215\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    214\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 215\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    217\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:85\u001b[0m, in \u001b[0;36m_clean_lower\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_clean_lower\u001b[39m(x):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# basic, remove whitespace, lower case\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlower()\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:67\u001b[0m, in \u001b[0;36mbasic_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbasic_clean\u001b[39m(text):\n\u001b[1;32m---> 67\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mftfy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\ftfy\\__init__.py:349\u001b[0m, in \u001b[0;36mfix_text\u001b[1;34m(text, config, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[1;32m--> 349\u001b[0m     textbreak \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m textbreak \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m         textbreak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n","\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'find'"]}],"source":["from torch.cuda.amp import autocast, GradScaler\n","import torch.optim as optim\n","\n","# Example optimizer\n","optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n","scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n","# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n","# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n","\n","for images, texts in dataloader:\n","    images, texts = images.to(device), texts.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n","        image_features, text_features = model(images, tokenizer(texts))\n","        loss = compute_loss(image_features, text_features)\n","\n","    scaler.scale(loss).backward()\n","    scaler.step(optimizer)\n","    scaler.update()\n"]},{"cell_type":"code","execution_count":null,"id":"7b0004e7-4f07-4a50-9e37-59898b465839","metadata":{"id":"7b0004e7-4f07-4a50-9e37-59898b465839"},"outputs":[],"source":["model = model.half() # FP32 -> FP16"]},{"cell_type":"code","execution_count":null,"id":"11093a0c-677b-4ad5-9dab-fb70bb48e5d4","metadata":{"id":"11093a0c-677b-4ad5-9dab-fb70bb48e5d4","outputId":"173ed6c6-45e4-4d54-c4d4-bfa532fdbea5"},"outputs":[{"ename":"NameError","evalue":"name 'image' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We must convert the input tensors?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m text_tokens\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"]}],"source":["# We must convert the input tensors?\n","image = image.half().to(device)\n","text_tokens = text_tokens.to(device)"]},{"cell_type":"markdown","id":"ea193656-26e6-4ba5-9270-3482dc1b00e0","metadata":{"id":"ea193656-26e6-4ba5-9270-3482dc1b00e0"},"source":["### Quantization (INT4)"]},{"cell_type":"code","execution_count":null,"id":"31439f7c-abbc-49d1-a3d8-05df7ecf84bf","metadata":{"id":"31439f7c-abbc-49d1-a3d8-05df7ecf84bf","outputId":"f1faaade-a7f0-4196-815a-8f76f4589d16"},"outputs":[{"ename":"PackageNotFoundError","evalue":"No package metadata was found for bitsandbytes","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)","File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:397\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","\u001b[1;31mStopIteration\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[1;32m----> 3\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-L-14\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_config\u001b[38;5;241m=\u001b[39mbnb_config)\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:510\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[1;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    508\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:568\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m ):\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m     )\n","File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:889\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n","File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:862\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mC:\\Python312\\Lib\\importlib\\metadata\\__init__.py:399\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n","\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"]}],"source":["from transformers import BitsAndBytesConfig\n","\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","model = open_clip.create_model(\"ViT-L-14\", quantized=True, bnb_config=bnb_config)\n"]},{"cell_type":"markdown","id":"ae070a66-ab08-49d4-97df-c822128640db","metadata":{"id":"ae070a66-ab08-49d4-97df-c822128640db"},"source":["### LoRA (Low-Rank Adaptation)"]},{"cell_type":"code","execution_count":null,"id":"6c312c62-04c6-42c9-aa40-91e75b6c2713","metadata":{"id":"6c312c62-04c6-42c9-aa40-91e75b6c2713"},"outputs":[],"source":["#This is the best option for fine-tuning CLIP:\n","# * Freeze the whole model\n","# * Insert small trainable LoRA layers\n","# * Train only 1–2% new parameters"]},{"cell_type":"code","execution_count":null,"id":"121df46c-3edd-4fe9-b803-ff1c7e30745e","metadata":{"id":"121df46c-3edd-4fe9-b803-ff1c7e30745e"},"outputs":[],"source":["#pip install peft"]},{"cell_type":"code","execution_count":null,"id":"8a86bac9-b859-4cf8-9ec4-bff3f8db841a","metadata":{"id":"8a86bac9-b859-4cf8-9ec4-bff3f8db841a","outputId":"e06d8614-72a0-461b-d25b-b927a52cb9f6"},"outputs":[{"ename":"AttributeError","evalue":"'CLIP' object has no attribute 'text'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[0;32m      3\u001b[0m lora_cfg \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m      4\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m      5\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      6\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      7\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# typical for transformer models\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m get_peft_model(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m, lora_cfg)\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n","\u001b[1;31mAttributeError\u001b[0m: 'CLIP' object has no attribute 'text'"]}],"source":["from peft import LoraConfig, get_peft_model\n","\n","lora_cfg = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=[\"q_proj\", \"v_proj\"],  # typical for transformer models\n",")\n","\n","model.text = get_peft_model(model.text, lora_cfg)\n","model.text.print_trainable_parameters()\n"]},{"cell_type":"code","execution_count":null,"id":"b761b122-a52a-41ee-b39f-6511957d11b2","metadata":{"id":"b761b122-a52a-41ee-b39f-6511957d11b2","outputId":"78fc307d-0f72-4797-a8ea-fcb4904a2db4"},"outputs":[{"name":"stdout","output_type":"stream","text":["CLIP(\n","  (visual): VisionTransformer(\n","    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","    (patch_dropout): Identity()\n","    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (transformer): Transformer(\n","      (resblocks): ModuleList(\n","        (0-11): 12 x ResidualAttentionBlock(\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ls_1): Identity()\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): GELU(approximate='none')\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ls_2): Identity()\n","        )\n","      )\n","    )\n","    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (resblocks): ModuleList(\n","      (0-11): 12 x ResidualAttentionBlock(\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ls_1): Identity()\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ls_2): Identity()\n","      )\n","    )\n","  )\n","  (token_embedding): Embedding(49408, 512)\n","  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","visual <class 'open_clip.transformer.VisionTransformer'>\n","transformer <class 'open_clip.transformer.Transformer'>\n","token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n","ln_final <class 'open_clip.transformer.LayerNorm'>\n","visual.ln_pre <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer <class 'open_clip.transformer.Transformer'>\n","visual.transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n","visual.transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","visual.transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","visual.transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","visual.transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","visual.transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","visual.transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n","visual.transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","visual.transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","visual.transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","visual.ln_post <class 'open_clip.transformer.LayerNorm'>\n","transformer <class 'open_clip.transformer.Transformer'>\n","transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n","transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n","transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n","transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n","transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n","transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n","transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n","transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n","transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n","transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n","token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n","ln_final <class 'open_clip.transformer.LayerNorm'>\n"]}],"source":["# print top-level modules\n","print(model)\n","\n","# print children names\n","for name, module in model.named_children():\n","    print(name, type(module))\n","\n","# print a few text-related submodules (common in open_clip)\n","for name, module in model.named_modules():\n","    if \"token\" in name or \"transformer\" in name or \"ln_\" in name or \"text\" in name:\n","        print(name, type(module))\n"]},{"cell_type":"code","execution_count":null,"id":"72ca34b6-57ed-40a6-9846-8509df6ab802","metadata":{"id":"72ca34b6-57ed-40a6-9846-8509df6ab802"},"outputs":[],"source":["token_emb = model.token_embedding\n","text_transformer = model.transformer\n","ln_final = model.ln_final   # or model.ln_post depending on the printout"]},{"cell_type":"code","execution_count":null,"id":"9b5f8ae0-12f0-4a1f-bc8b-45b7fd8fb291","metadata":{"id":"9b5f8ae0-12f0-4a1f-bc8b-45b7fd8fb291","outputId":"faf569ab-92df-4208-e80f-9f8460af54a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable params: 0\n"]}],"source":["# reeze the whole model (prepare for LoRA)\n","#Usually you want to freeze the base model and train only small adapter parameters:\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Trainable params:\", trainable)\n"]},{"cell_type":"code","execution_count":null,"id":"9357a9ef-9eb8-4315-952c-046492c5ef5e","metadata":{"id":"9357a9ef-9eb8-4315-952c-046492c5ef5e","outputId":"ea499965-ea33-4c11-b6c8-a42468dfc270"},"outputs":[{"name":"stdout","output_type":"stream","text":["Replaced resblocks.0.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.1.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.2.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.3.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.4.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.5.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.6.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.7.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.8.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.9.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.10.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Replaced resblocks.11.attn.out_proj with LoRA Linear (r=8, alpha=32)\n","Trainable params: 98304 / 149719041\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\zhesh\\AppData\\Local\\Temp\\ipykernel_32216\\2378032752.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]},{"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'find'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[21], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, texts \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     74\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 75\u001b[0m     text_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:256\u001b[0m, in \u001b[0;36mSimpleTokenizer.__call__\u001b[1;34m(self, texts, context_length)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# use reduction strategy for tokenize if set, otherwise default to truncation below\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn(\n\u001b[0;32m    249\u001b[0m         texts,\n\u001b[0;32m    250\u001b[0m         context_length\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         encode_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode,\n\u001b[0;32m    254\u001b[0m     )\n\u001b[1;32m--> 256\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_token_id] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meot_token_id] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:215\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    214\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 215\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    217\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:85\u001b[0m, in \u001b[0;36m_clean_lower\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_clean_lower\u001b[39m(x):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# basic, remove whitespace, lower case\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlower()\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:67\u001b[0m, in \u001b[0;36mbasic_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbasic_clean\u001b[39m(text):\n\u001b[1;32m---> 67\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mftfy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n","File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\ftfy\\__init__.py:349\u001b[0m, in \u001b[0;36mfix_text\u001b[1;34m(text, config, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[1;32m--> 349\u001b[0m     textbreak \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m textbreak \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m         textbreak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n","\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'find'"]}],"source":["#Add LoRA adapters to transformer linear layers\n","#pip install loralib\n","\n","# peft is primarily designed for Hugging Face models. For OpenCLIP, a simple and compatible approach is to use loralib (lightweight LoRA wrapper)\n","\n","### Wrap target Linear layers (q/k/v projections or all Linear)\n","\n","import torch.nn as nn\n","import loralib as lora\n","\n","# Example helper: wrap Linear modules within a module whose name contains 'transformer'\n","def apply_lora_to_transformer(root_module, r=8, alpha=32, target_module_type=nn.Linear, name_filter=None):\n","    \"\"\"\n","    Wrap Linear layers inside root_module with LoRA.\n","    - r, alpha: LoRA hyperparams\n","    - name_filter: optional substring to filter which modules to wrap (e.g. 'attn' or 'q_proj')\n","    \"\"\"\n","    for name, mod in root_module.named_modules():\n","        # We only want to wrap the *leaf* Linear modules, not the parent modules\n","        if isinstance(mod, target_module_type):\n","            if name_filter is None or name_filter in name:\n","                parent_path = name.rsplit('.', 1)[0] if '.' in name else ''\n","                # rebind module in parent\n","                parent = root_module\n","                if parent_path:\n","                    for part in parent_path.split('.'):\n","                        parent = getattr(parent, part)\n","                attr_name = name.split('.')[-1]\n","                orig = getattr(parent, attr_name)\n","                # create LoRA-wrapped layer with same in/out dims\n","                lora_layer = lora.Linear(orig.in_features, orig.out_features, r=r, lora_alpha=alpha, bias=(orig.bias is not None))\n","                # copy weight and bias\n","                lora_layer.weight.data = orig.weight.data.clone()\n","                if orig.bias is not None:\n","                    lora_layer.bias.data = orig.bias.data.clone()\n","                # replace\n","                setattr(parent, attr_name, lora_layer)\n","                print(f\"Replaced {name} with LoRA Linear (r={r}, alpha={alpha})\")\n","\n","# Apply to the text transformer\n","apply_lora_to_transformer(model.transformer, r=8, alpha=32, name_filter=\"attn\")  # focus on attention proj\n","\n","\n","# name_filter helps target q_proj, k_proj, v_proj, or modules with attn in the path.\n","#Inspect your printed module names to choose an appropriate filter.\n","\n","# This replaces nn.Linear objects with loralib.Linear that contain LoRA parameters;\n","# those LoRA parameters will be trainable while base weights remain frozen (unless you unfreeze them).\n","\n","### Make LoRA params trainable and check\n","\n","# Ensure base params still frozen, LoRA params trainable\n","for name, p in model.named_parameters():\n","    if \"lora\" in name.lower() or \"lora\" in name:\n","        p.requires_grad = True\n","    else:\n","        p.requires_grad = False\n","\n","# Print trainable params count\n","trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total = sum(p.numel() for p in model.parameters())\n","print(f\"Trainable params: {trainable} / {total}\")\n","\n","### Training loop: mixed precision + optimizer\n","\n","from torch.cuda.amp import autocast, GradScaler\n","optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n","scaler = GradScaler()\n","\n","model.to(device)\n","model.train()\n","\n","for images, texts in dataloader:\n","    images = images.to(device)\n","    text_tokens = tokenizer(texts).to(device)\n","\n","    optimizer.zero_grad()\n","    with autocast():\n","        image_features = model.encode_image(images)   # or model.visual(images)\n","        text_features = model.encode_text(text_tokens) # or model.transformer(...)\n","        loss = compute_loss(image_features, text_features)\n","\n","    scaler.scale(loss).backward()\n","    scaler.step(optimizer)\n","    scaler.update()\n"]},{"cell_type":"code","execution_count":null,"id":"7291c277-d8ba-4075-9c98-c71878b79f72","metadata":{"id":"7291c277-d8ba-4075-9c98-c71878b79f72"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"75ec9dbb-daf0-4ad0-a635-d418fbe390c7","metadata":{"ExecuteTime":{"end_time":"2025-10-27T21:22:09.711655Z","start_time":"2025-10-27T21:22:09.695004Z"},"editable":true,"tags":[],"id":"75ec9dbb-daf0-4ad0-a635-d418fbe390c7"},"outputs":[],"source":["# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n","s3 = boto3.client(\n","    \"s3\",\n","    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n","    aws_access_key_id=\"minioadmin\", # User name\n","    aws_secret_access_key=\"minioadmin\", # Password\n",")"]},{"cell_type":"code","execution_count":null,"id":"38b94b12-3dde-4ab1-bd66-364e4bfeef55","metadata":{"ExecuteTime":{"end_time":"2025-10-27T21:22:53.421060Z","start_time":"2025-10-27T21:22:52.802581Z"},"id":"38b94b12-3dde-4ab1-bd66-364e4bfeef55"},"outputs":[],"source":["# Connect to the server (Docker Container)\n","client = chromadb.HttpClient(host=\"localhost\", port=8000)\n","\n","# Create or get the collection named \"texts_images\" to store embeddings of images and texts\n","collection_texts_images = client.create_collection(name=\"texts_images\", get_or_create=True, embedding_function=None)"]},{"cell_type":"code","execution_count":null,"id":"6530a273-faa1-4600-94d3-f0e6f2fd172e","metadata":{"scrolled":true,"id":"6530a273-faa1-4600-94d3-f0e6f2fd172e","outputId":"ac7b0f3c-5987-4830-d7b1-e83a80756675"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bucket 'training-data-construction-zone' already exists!\n"]},{"data":{"text/plain":["{'ResponseMetadata': {'RequestId': '1877A5207DB01D9E',\n","  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n","  'HTTPStatusCode': 200,\n","  'HTTPHeaders': {'accept-ranges': 'bytes',\n","   'content-length': '0',\n","   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n","   'server': 'MinIO',\n","   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n","   'vary': 'Origin, Accept-Encoding',\n","   'x-amz-checksum-crc32': 'AAAAAA==',\n","   'x-amz-checksum-type': 'FULL_OBJECT',\n","   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n","   'x-amz-request-id': '1877A5207DB01D9E',\n","   'x-content-type-options': 'nosniff',\n","   'x-ratelimit-limit': '2107',\n","   'x-ratelimit-remaining': '2107',\n","   'x-xss-protection': '1; mode=block',\n","   'date': 'Thu, 13 Nov 2025 18:42:18 GMT'},\n","  'RetryAttempts': 0},\n"," 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n"," 'ChecksumCRC32': 'AAAAAA==',\n"," 'ChecksumType': 'FULL_OBJECT'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# We create a new Bucket in Min-IO to store our training data\n","\n","# List existing buckets\n","buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n","\n","# Function that given a name, creates a bucket\n","def createBucket(name, list_buckets):\n","    if name in list_buckets:\n","        print(f\"Bucket '{name}' already exists!\")\n","    else:\n","        s3.create_bucket(Bucket=name)\n","        print(f\"Created bucket: {name}\")\n","\n","# Create a bucket named landing_zone\n","createBucket(\"training-data-construction-zone\", buckets)\n","# Sub-bucket: Baseline Training Data\n","s3.put_object(Bucket=\"training-data-construction-zone\", Key=\"baseline-training-data/\")"]},{"cell_type":"code","execution_count":null,"id":"ebb0a71f-ce37-46a4-a82a-e4561e96e452","metadata":{"scrolled":true,"id":"ebb0a71f-ce37-46a4-a82a-e4561e96e452","outputId":"726ba567-a95b-48fb-ba7b-6281ac7fb112"},"outputs":[{"data":{"text/plain":["CLIP(\n","  (visual): VisionTransformer(\n","    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n","    (patch_dropout): Identity()\n","    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (transformer): Transformer(\n","      (resblocks): ModuleList(\n","        (0-23): 24 x ResidualAttentionBlock(\n","          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (ls_1): Identity()\n","          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n","            (gelu): GELU(approximate='none')\n","            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n","          )\n","          (ls_2): Identity()\n","        )\n","      )\n","    )\n","    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (resblocks): ModuleList(\n","      (0-11): 12 x ResidualAttentionBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (ls_1): Identity()\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (ls_2): Identity()\n","      )\n","    )\n","  )\n","  (token_embedding): Embedding(49408, 768)\n","  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Just in case our device has gpu\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Load model\n","model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n","tokenizer = open_clip.get_tokenizer(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\") # Tokenizer for texts\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"id":"5ec0fb2f-fc84-4300-910d-8ea911ed1e19","metadata":{"id":"5ec0fb2f-fc84-4300-910d-8ea911ed1e19"},"outputs":[],"source":["# Some helper functions\n","\n","# We can use this function to retrieve an text from our bucket\n","def get_text(bucket, key):\n","    resp = s3.get_object(Bucket=bucket, Key=key)\n","    body = resp[\"Body\"].read()\n","    text = body.decode(\"utf-8\")\n","    return text\n","@torch.no_grad()\n","# The next function returns the embedding of the given text\n","def embed_text(model, tokenizer, texts: str):\n","    tokens = tokenizer([texts]).to(device) # tokenized batch\n","    feats = model.encode_text(tokens)\n","    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n","    return feats.cpu().numpy()[0]"]},{"cell_type":"code","execution_count":null,"id":"313a798d-b5cb-4764-b141-647bf5ba6e6b","metadata":{"id":"313a798d-b5cb-4764-b141-647bf5ba6e6b"},"outputs":[],"source":["# This function performs a similarity search for each text description in the dataset\n","# to retrieve the most similar image, forming image–text pairs for training.\n","def baseline_training_data_generator(src_bucket, dest_bucket, collection, model_text, tokenizer, src_prefix=\"texts/\", dest_prefix=\"baseline-training-data/\"):\n","\n","    # Incremental id assigned to each image-text pair\n","    id_counter = 0\n","\n","    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n","    for page in paginator.paginate(Bucket=src_bucket, Prefix=src_prefix):\n","\n","        # List of paths (meta_data)\n","        image_paths = []\n","        # List of embeddings\n","        embeddings = []\n","        # List of unique IDs for each embedding\n","        ids = []\n","\n","        for obj in page.get(\"Contents\", []):\n","\n","            key = obj[\"Key\"]\n","\n","            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n","                continue\n","\n","            id_counter += 1\n","\n","            # Get the description\n","            description = get_text(src_bucket, key)\n","            # Get the embeddings of the description\n","            q_vec = embed_text(model_text, tokenizer, description)\n","            # Apply the similarity search using the description\n","            res_image = collection.query(\n","                query_embeddings=[q_vec],\n","                n_results=1,\n","                where={\"type\": \"image\"}, # Filter by metadata type\n","                include=[\"documents\", \"distances\"]\n","            )\n","            # Get the key for the image\n","            key_image = res_image['documents'][0][0][len(src_bucket) + 1:]\n","\n","            # Remove the prefix part from the key\n","            new_key_text = dest_prefix + \"text_\" + str(id_counter).zfill(6) + \".txt\" # ids of 000001, 000002, ...\n","            new_key_image = dest_prefix + \"image_\" + str(id_counter).zfill(6) + \".png\" # ids of 000001, 000002, ...\n","\n","            # Copy objects without top-level folder and rename them\n","            copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n","            copy_source_image = {\"Bucket\": src_bucket, \"Key\": key_image}\n","            s3.copy_object(Bucket=dest_bucket, Key=new_key_text, CopySource=copy_source_text)\n","            s3.copy_object(Bucket=dest_bucket, Key=new_key_image, CopySource=copy_source_image)\n","\n","            print(f\"✅ Baseline training pair #{id_counter} created successfully.\")\n","\n","    print(f\"✅ All training pairs have been successfully created.\")"]},{"cell_type":"code","execution_count":null,"id":"790ffa5b-6629-4ca1-ad0f-dd5dd9afa495","metadata":{"id":"790ffa5b-6629-4ca1-ad0f-dd5dd9afa495"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"968b19cc6bff473399650af34399a861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10d8c1efdd734b46979695b4403f6531","IPY_MODEL_f0401207902845fe94731ec4ea31f1a4","IPY_MODEL_5602826c956f46109b394bccb0ba96a5"],"layout":"IPY_MODEL_d87485205b5f4566a7b045ca2ca8c2eb"}},"10d8c1efdd734b46979695b4403f6531":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57011c8286484fd4b51bd40594dd4ac7","placeholder":"​","style":"IPY_MODEL_c04808686d6b49ef8ff355491e9d2446","value":"open_clip_model.safetensors: 100%"}},"f0401207902845fe94731ec4ea31f1a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4946a822da74e67b2ec65cb9cf181cf","max":598516980,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c2698ed7db84e589fb5f3f6853cf2e3","value":598516980}},"5602826c956f46109b394bccb0ba96a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f57afb85a7c04e3583639fb73e157a55","placeholder":"​","style":"IPY_MODEL_4c22da1870774f3bbafed1360b13fd2c","value":" 599M/599M [00:08&lt;00:00, 149MB/s]"}},"d87485205b5f4566a7b045ca2ca8c2eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57011c8286484fd4b51bd40594dd4ac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c04808686d6b49ef8ff355491e9d2446":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4946a822da74e67b2ec65cb9cf181cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c2698ed7db84e589fb5f3f6853cf2e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f57afb85a7c04e3583639fb73e157a55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c22da1870774f3bbafed1360b13fd2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}