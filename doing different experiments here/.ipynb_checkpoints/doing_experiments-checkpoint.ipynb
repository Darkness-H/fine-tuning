{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cefc238-8601-4418-a77c-10c2b7c198a3",
   "metadata": {},
   "source": [
    "## ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81cc0dc-4227-44a0-81f1-5944c47ca52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:10.758994Z",
     "start_time": "2025-10-27T21:22:10.754679Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1919, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Temp\\ipykernel_5492\\790553134.py\", line 7, in <module>\n",
      "    import open_clip\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\__init__.py\", line 3, in <module>\n",
      "    from .coca_model import CoCa\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\coca_model.py\", line 15, in <module>\n",
      "    from .model import CLIPTextCfg, CLIPVisionCfg, _build_vision_tower, _build_text_tower\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\model.py\", line 18, in <module>\n",
      "    from .hf_model import HFTextEncoder\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\hf_model.py\", line 13, in <module>\n",
      "    from transformers import AutoModel, AutoTokenizer, AutoConfig, PretrainedConfig\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2317, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2345, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py\", line 23, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 43, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2317, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2345, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 55, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py\", line 29, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 52, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import torch\n",
    "import boto3\n",
    "import random\n",
    "import chromadb\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 10721\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f6387f5-9efc-4d28-9dd7-543c425afbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c4c9b8-7890-41fb-b2c3-b68b47378ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP ViT-L/16\n",
    "model_name = \"ViT-B-16\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d5b8c4-eee2-47b5-a9c6-bb84051af30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CLIP-ViT-B-16\n",
      "Total parameters: 149,620,737\n",
      "Trainable parameters: 149,620,737\n",
      "The parameters are in: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# ---- Show parameter counts ----\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: CLIP-ViT-B-16\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"The parameters are in: {str(next(model.parameters()).dtype)}\") # FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad48e98-d882-4d9d-a6a0-962ad0a1b109",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "743c2079-58a7-46c5-892e-9745549e28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import functools\n",
    "from torchvision import transforms\n",
    "from typing import List, Tuple\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data_bucket, data_prefix, s3):\n",
    "        self.data_bucket = data_bucket\n",
    "        self.data_prefix = data_prefix\n",
    "        self.s3 = s3\n",
    "\n",
    "        # Data keys\n",
    "        self.image_keys, self.text_keys = self.__loadfromminio__(data_bucket, data_prefix)\n",
    "\n",
    "    def __loadfromminio__(self, data_bucket, data_prefix):\n",
    "        image_keys = []\n",
    "        text_keys = []\n",
    "        paginator = self.s3.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=data_bucket, Prefix=data_prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if obj['Size'] == 0 and key.endswith(\"/\"):\n",
    "                    continue\n",
    "                if \"image\" in key.split(\"/\")[1]: # We only need images to find their corresponding description in MinIO\n",
    "                    image_keys.append(key)\n",
    "                    text_key = data_prefix + key.split(\"/\")[1].replace(\"image\", \"text\").replace(\"png\", \"txt\")\n",
    "                    text_keys.append(text_key)\n",
    "\n",
    "        # From lists to arrays\n",
    "        image_keys = np.array(image_keys)\n",
    "        text_keys = np.array(text_keys)\n",
    "        \n",
    "        return image_keys, text_keys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_keys)\n",
    "\n",
    "    def __getfile__(self, data_bucket, key, filetype = \"image\"):\n",
    "        resp = self.s3.get_object(Bucket=data_bucket, Key=key)\n",
    "        body = resp[\"Body\"].read()\n",
    "        if filetype == \"image\":\n",
    "            file = Image.open(BytesIO(body))\n",
    "        else: # filetype = \"text\"\n",
    "            file = body.decode(\"utf-8\")\n",
    "        return file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load image\n",
    "        image = self.__getfile__(self.data_bucket, self.image_keys[idx], filetype = \"image\")\n",
    "        \n",
    "        # Load text\n",
    "        text = self.__getfile__(self.data_bucket, self.text_keys[idx], filetype = \"text\")\n",
    "\n",
    "        return image, text\n",
    "\n",
    "# Collate function that applies preprocess and tokenizer to the batch\n",
    "def collate_fn(batch: List[Tuple[\"PIL.Image.Image\", str]], preprocess, tokenizer, pad_value: int = 0):\n",
    "    \"\"\"\n",
    "    batch: list of (PIL.Image, text_str)\n",
    "    preprocess: image preprocessing transform (from open_clip.create_model_and_transforms)\n",
    "    tokenizer: open_clip tokenizer callable\n",
    "    pad_value: value used to pad token sequences (default 0)\n",
    "\n",
    "    Returns:\n",
    "        images: torch.Tensor [B, C, H, W]\n",
    "        text_tokens: torch.LongTensor [B, L]\n",
    "        raw_texts: list[str]\n",
    "    \"\"\"\n",
    "    \n",
    "    images_pil, raw_texts = zip(*batch) # tuples\n",
    "\n",
    "    # --- Images: apply model-specific preprocess (PIL->Tensor) and stack ---\n",
    "    images = [preprocess(img) for img in images_pil] # each should be a Tensor\n",
    "    images = torch.stack(images, dim=0) # [B, C, H, W]\n",
    "\n",
    "    # --- Texts: use tokenizer ---\n",
    "    # Many open_clip tokenizers accept a list[str] and return a torch.LongTensor [B, L].\n",
    "    # But sometimes they may return a list of tensors or lists. Handle both cases.\n",
    "    tokenized = tokenizer(raw_texts) # [B, L]\n",
    "\n",
    "    return images, tokenized\n",
    "\n",
    "# Wrap collate_fn so DataLoader only sees a single-argument function\n",
    "collate = functools.partial(collate_fn, preprocess=preprocess, tokenizer=tokenizer, pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94cd9bd2-0882-4593-9de8-aae65d949e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customized dataset object\n",
    "baseline_dataset = ImageTextDataset(\n",
    "    data_bucket = \"training-data-construction-zone\",\n",
    "    data_prefix = \"baseline-training-data/\",\n",
    "    s3 = s3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85188741-0c13-4053-8465-ec2651bb4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a shuffle over the dataset to prevent the model from learning order-based patterns\n",
    "dataloader = DataLoader(baseline_dataset, batch_size=16, shuffle=True, collate_fn=collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c752e4-d951-4194-bdc2-54de17a5a210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SakuraSnow\\AppData\\Local\\Temp\\ipykernel_5492\\242368609.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
      "C:\\Users\\SakuraSnow\\AppData\\Local\\Temp\\ipykernel_5492\\242368609.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Test fine-tuning on ViT-B-16 with Mixed/Reduced Precision (FP16)\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
    "# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n",
    "# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n",
    "        image_features, text_features = model(images, texts)\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c234c0-3e7b-44d8-979e-6ffb9f8c7bb2",
   "metadata": {},
   "source": [
    "### Mixed/Reduced Precision (FP16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b36c70-14b5-4e04-9541-71854b305a31",
   "metadata": {},
   "source": [
    "Memory usage is roughly halved, and training is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a556b65-16dd-42a6-9434-a75462881703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b2571-41e5-4617-bc73-2a7f4004ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler() # For automatic mixed precision, prevents gradient underflow\n",
    "# Gradient underflow: FP16 has smaller numeric range than FP32. Very small gradients may become so tiny that FP16 rounds them to zero.\n",
    "# This is called underflow, and it effectively \"kills\" the learning signal for some parameters.\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images, texts = images.to(device), texts.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast(): # the autocast() context ensures operations automatically use FP16 where safe\n",
    "        image_features, text_features = model(images, tokenizer(texts))\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0004e7-4f07-4a50-9e37-59898b465839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half() # FP32 -> FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11093a0c-677b-4ad5-9dab-fb70bb48e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must convert the input tensors?\n",
    "image = image.half().to(device)\n",
    "text_tokens = text_tokens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea193656-26e6-4ba5-9270-3482dc1b00e0",
   "metadata": {},
   "source": [
    "### Quantization (INT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31439f7c-abbc-49d1-a3d8-05df7ecf84bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = open_clip.create_model(\"ViT-L-14\", quantized=True, bnb_config=bnb_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae070a66-ab08-49d4-97df-c822128640db",
   "metadata": {},
   "source": [
    "### LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c312c62-04c6-42c9-aa40-91e75b6c2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the best option for fine-tuning CLIP:\n",
    "# * Freeze the whole model\n",
    "# * Insert small trainable LoRA layers\n",
    "# * Train only 1â€“2% new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121df46c-3edd-4fe9-b803-ff1c7e30745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86bac9-b859-4cf8-9ec4-bff3f8db841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # typical for transformer models\n",
    ")\n",
    "\n",
    "model.text = get_peft_model(model.text, lora_cfg)\n",
    "model.text.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b761b122-a52a-41ee-b39f-6511957d11b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (visual): VisionTransformer(\n",
      "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (patch_dropout): Identity()\n",
      "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): ModuleList(\n",
      "        (0-11): 12 x ResidualAttentionBlock(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (ls_1): Identity()\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (gelu): GELU(approximate='none')\n",
      "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (ls_2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (resblocks): ModuleList(\n",
      "      (0-11): 12 x ResidualAttentionBlock(\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ls_1): Identity()\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (ls_2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "visual <class 'open_clip.transformer.VisionTransformer'>\n",
      "transformer <class 'open_clip.transformer.Transformer'>\n",
      "token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ln_final <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.ln_pre <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer <class 'open_clip.transformer.Transformer'>\n",
      "visual.transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "visual.transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "visual.transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "visual.transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "visual.transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "visual.transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "visual.transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "visual.transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "visual.transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "visual.ln_post <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer <class 'open_clip.transformer.Transformer'>\n",
      "transformer.resblocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "transformer.resblocks.0 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.0.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.0.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.0.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.0.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.0.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.0.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.0.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.0.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.0.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.0.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.1 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.1.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.1.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.1.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.1.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.1.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.1.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.1.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.1.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.1.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.1.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.2 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.2.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.2.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.2.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.2.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.2.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.2.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.2.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.2.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.2.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.2.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.3 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.3.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.3.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.3.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.3.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.3.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.3.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.3.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.3.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.3.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.3.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.4 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.4.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.4.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.4.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.4.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.4.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.4.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.4.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.4.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.4.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.4.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.5 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.5.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.5.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.5.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.5.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.5.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.5.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.5.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.5.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.5.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.5.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.6 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.6.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.6.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.6.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.6.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.6.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.6.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.6.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.6.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.6.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.6.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.7 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.7.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.7.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.7.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.7.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.7.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.7.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.7.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.7.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.7.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.7.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.8 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.8.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.8.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.8.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.8.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.8.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.8.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.8.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.8.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.8.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.8.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.9 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.9.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.9.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.9.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.9.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.9.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.9.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.9.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.9.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.9.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.9.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.10 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.10.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.10.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.10.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.10.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.10.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.10.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.10.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.10.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.10.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.10.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.11 <class 'open_clip.transformer.ResidualAttentionBlock'>\n",
      "transformer.resblocks.11.ln_1 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.11.attn <class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "transformer.resblocks.11.attn.out_proj <class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "transformer.resblocks.11.ls_1 <class 'torch.nn.modules.linear.Identity'>\n",
      "transformer.resblocks.11.ln_2 <class 'open_clip.transformer.LayerNorm'>\n",
      "transformer.resblocks.11.mlp <class 'torch.nn.modules.container.Sequential'>\n",
      "transformer.resblocks.11.mlp.c_fc <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.11.mlp.gelu <class 'torch.nn.modules.activation.GELU'>\n",
      "transformer.resblocks.11.mlp.c_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "transformer.resblocks.11.ls_2 <class 'torch.nn.modules.linear.Identity'>\n",
      "token_embedding <class 'torch.nn.modules.sparse.Embedding'>\n",
      "ln_final <class 'open_clip.transformer.LayerNorm'>\n"
     ]
    }
   ],
   "source": [
    "# print top-level modules\n",
    "print(model)\n",
    "\n",
    "# print children names\n",
    "for name, module in model.named_children():\n",
    "    print(name, type(module))\n",
    "\n",
    "# print a few text-related submodules (common in open_clip)\n",
    "for name, module in model.named_modules():\n",
    "    if \"token\" in name or \"transformer\" in name or \"ln_\" in name or \"text\" in name:\n",
    "        print(name, type(module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72ca34b6-57ed-40a6-9846-8509df6ab802",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_emb = model.token_embedding\n",
    "text_transformer = model.transformer\n",
    "ln_final = model.ln_final   # or model.ln_post depending on the printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b5f8ae0-12f0-4a1f-bc8b-45b7fd8fb291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "# reeze the whole model (prepare for LoRA)\n",
    "#Usually you want to freeze the base model and train only small adapter parameters:\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable params:\", trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357a9ef-9eb8-4315-952c-046492c5ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add LoRA adapters to transformer linear layers\n",
    "#pip install loralib\n",
    "\n",
    "# peft is primarily designed for Hugging Face models. For OpenCLIP, a simple and compatible approach is to use loralib (lightweight LoRA wrapper)\n",
    "\n",
    "### Wrap target Linear layers (q/k/v projections or all Linear)\n",
    "\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "\n",
    "# Example helper: wrap Linear modules within a module whose name contains 'transformer'\n",
    "def apply_lora_to_transformer(root_module, r=8, alpha=32, target_module_type=nn.Linear, name_filter=None):\n",
    "    \"\"\"\n",
    "    Wrap Linear layers inside root_module with LoRA.\n",
    "    - r, alpha: LoRA hyperparams\n",
    "    - name_filter: optional substring to filter which modules to wrap (e.g. 'attn' or 'q_proj')\n",
    "    \"\"\"\n",
    "    for name, mod in root_module.named_modules():\n",
    "        # We only want to wrap the *leaf* Linear modules, not the parent modules\n",
    "        if isinstance(mod, target_module_type):\n",
    "            if name_filter is None or name_filter in name:\n",
    "                parent_path = name.rsplit('.', 1)[0] if '.' in name else ''\n",
    "                # rebind module in parent\n",
    "                parent = root_module\n",
    "                if parent_path:\n",
    "                    for part in parent_path.split('.'):\n",
    "                        parent = getattr(parent, part)\n",
    "                attr_name = name.split('.')[-1]\n",
    "                orig = getattr(parent, attr_name)\n",
    "                # create LoRA-wrapped layer with same in/out dims\n",
    "                lora_layer = lora.Linear(orig.in_features, orig.out_features, r=r, lora_alpha=alpha, bias=(orig.bias is not None))\n",
    "                # copy weight and bias\n",
    "                lora_layer.weight.data = orig.weight.data.clone()\n",
    "                if orig.bias is not None:\n",
    "                    lora_layer.bias.data = orig.bias.data.clone()\n",
    "                # replace\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "                print(f\"Replaced {name} with LoRA Linear (r={r}, alpha={alpha})\")\n",
    "\n",
    "# Apply to the text transformer\n",
    "apply_lora_to_transformer(model.transformer, r=8, alpha=32, name_filter=\"attn\")  # focus on attention proj\n",
    "\n",
    "\n",
    "# name_filter helps target q_proj, k_proj, v_proj, or modules with attn in the path.\n",
    "#Inspect your printed module names to choose an appropriate filter.\n",
    "\n",
    "# This replaces nn.Linear objects with loralib.Linear that contain LoRA parameters;\n",
    "# those LoRA parameters will be trainable while base weights remain frozen (unless you unfreeze them).\n",
    "\n",
    "### Make LoRA params trainable and check\n",
    "\n",
    "# Ensure base params still frozen, LoRA params trainable\n",
    "for name, p in model.named_parameters():\n",
    "    if \"lora\" in name.lower() or \"lora\" in name:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Print trainable params count\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total}\")\n",
    "\n",
    "### Training loop: mixed precision + optimizer\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for images, texts in dataloader:\n",
    "    images = images.to(device)\n",
    "    text_tokens = tokenizer(texts).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        image_features = model.encode_image(images)   # or model.visual(images)\n",
    "        text_features = model.encode_text(text_tokens) # or model.transformer(...)\n",
    "        loss = compute_loss(image_features, text_features)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291c277-d8ba-4075-9c98-c71878b79f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec9dbb-daf0-4ad0-a635-d418fbe390c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:09.711655Z",
     "start_time": "2025-10-27T21:22:09.695004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b94b12-3dde-4ab1-bd66-364e4bfeef55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T21:22:53.421060Z",
     "start_time": "2025-10-27T21:22:52.802581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to the server (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Create or get the collection named \"texts_images\" to store embeddings of images and texts\n",
    "collection_texts_images = client.create_collection(name=\"texts_images\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6530a273-faa1-4600-94d3-f0e6f2fd172e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'training-data-construction-zone' already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1877A5207DB01D9E',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'AAAAAA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '1877A5207DB01D9E',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2107',\n",
       "   'x-ratelimit-remaining': '2107',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Thu, 13 Nov 2025 18:42:18 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ChecksumCRC32': 'AAAAAA==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new Bucket in Min-IO to store our training data\n",
    "\n",
    "# List existing buckets\n",
    "buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n",
    "\n",
    "# Function that given a name, creates a bucket\n",
    "def createBucket(name, list_buckets):\n",
    "    if name in list_buckets:\n",
    "        print(f\"Bucket '{name}' already exists!\")\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=name)\n",
    "        print(f\"Created bucket: {name}\")\n",
    "\n",
    "# Create a bucket named landing_zone\n",
    "createBucket(\"training-data-construction-zone\", buckets)\n",
    "# Sub-bucket: Baseline Training Data\n",
    "s3.put_object(Bucket=\"training-data-construction-zone\", Key=\"baseline-training-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb0a71f-ce37-46a4-a82a-e4561e96e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
    "tokenizer = open_clip.get_tokenizer(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\") # Tokenizer for texts\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ec0fb2f-fc84-4300-910d-8ea911ed1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "\n",
    "# We can use this function to retrieve an text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text\n",
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "313a798d-b5cb-4764-b141-647bf5ba6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a similarity search for each text description in the dataset \n",
    "# to retrieve the most similar image, forming imageâ€“text pairs for training.\n",
    "def baseline_training_data_generator(src_bucket, dest_bucket, collection, model_text, tokenizer, src_prefix=\"texts/\", dest_prefix=\"baseline-training-data/\"):\n",
    "\n",
    "    # Incremental id assigned to each image-text pair\n",
    "    id_counter = 0\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=src_prefix):\n",
    "\n",
    "        # List of paths (meta_data)\n",
    "        image_paths = []\n",
    "        # List of embeddings\n",
    "        embeddings = []\n",
    "        # List of unique IDs for each embedding\n",
    "        ids = []\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "            # Get the description\n",
    "            description = get_text(src_bucket, key)\n",
    "            # Get the embeddings of the description\n",
    "            q_vec = embed_text(model_text, tokenizer, description)\n",
    "            # Apply the similarity search using the description\n",
    "            res_image = collection.query(\n",
    "                query_embeddings=[q_vec],\n",
    "                n_results=1,\n",
    "                where={\"type\": \"image\"}, # Filter by metadata type\n",
    "                include=[\"documents\", \"distances\"]\n",
    "            )\n",
    "            # Get the key for the image\n",
    "            key_image = res_image['documents'][0][0][len(src_bucket) + 1:]\n",
    "\n",
    "            # Remove the prefix part from the key\n",
    "            new_key_text = dest_prefix + \"text_\" + str(id_counter).zfill(6) + \".txt\" # ids of 000001, 000002, ...\n",
    "            new_key_image = dest_prefix + \"image_\" + str(id_counter).zfill(6) + \".png\" # ids of 000001, 000002, ...\n",
    "\n",
    "            # Copy objects without top-level folder and rename them\n",
    "            copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "            copy_source_image = {\"Bucket\": src_bucket, \"Key\": key_image}\n",
    "            s3.copy_object(Bucket=dest_bucket, Key=new_key_text, CopySource=copy_source_text)\n",
    "            s3.copy_object(Bucket=dest_bucket, Key=new_key_image, CopySource=copy_source_image)\n",
    "\n",
    "            print(f\"âœ… Baseline training pair #{id_counter} created successfully.\")\n",
    "\n",
    "    print(f\"âœ… All training pairs have been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ffa5b-6629-4ca1-ad0f-dd5dd9afa495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
