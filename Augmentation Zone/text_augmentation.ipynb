{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1513906-5c7c-4525-abc1-0067f560c81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/60/90/81ac364ef94209c100e12579629dc92bf7a709a84af32f8c551b02c07e94/nltk-3.9.2-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.7 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install nlpaug\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eba5bec-aa97-45d6-ad6a-492a2e432f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import boto3\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "import re\n",
    "from nlpaug.util import Action\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6150ca19-7cf1-4cba-b9ff-ffd20cfd4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053b671e-4df8-4153-a454-cd1e1000b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'augmentation-zone' already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '187915C0DA2AB6D2',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'AAAAAA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '187915C0DA2AB6D2',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2110',\n",
       "   'x-ratelimit-remaining': '2110',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Tue, 18 Nov 2025 11:17:27 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ChecksumCRC32': 'AAAAAA==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new Bucket in Min-IO to store our augmented data\n",
    "\n",
    "# List existing buckets\n",
    "buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n",
    "\n",
    "# Function that given a name, creates a bucket\n",
    "def createBucket(name, list_buckets):\n",
    "    if name in list_buckets:\n",
    "        print(f\"Bucket '{name}' already exists!\")\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=name)\n",
    "        print(f\"Created bucket: {name}\")\n",
    "\n",
    "# Create a bucket named landing_zone\n",
    "createBucket(\"augmentation-zone\", buckets)\n",
    "# Sub-bucket: Baseline Training Data\n",
    "s3.put_object(Bucket=\"augmentation-zone\", Key=\"texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf5e4b2-bfd7-43e9-919e-ecf30285e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddd73f5-2f19-427f-8e3f-ae53f84a259b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
    "tokenizer = open_clip.get_tokenizer(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\") # Tokenizer for texts\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d6eedd-e573-42a8-8ccf-d449c481ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c5bc29d-ef67-4156-a00e-aa30dc3c7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_augmentation(src_bucket, dest_bucket, model_text, tokenizer, dest_prefix=\"texts/\"):\n",
    "    # Incremental id assigned to each image-text pair\n",
    "    id_counter = 0\n",
    "    stopwords = [ 'Epics', 'Steam'] # don't change name of plataform\n",
    "    stopwords_regex = r'\\d+(\\.\\d+)?'  # don't change numbers\n",
    "    augsim = naw.SynonymAug(\n",
    "        aug_src='wordnet',\n",
    "        lang='eng',\n",
    "        aug_p=0.3,\n",
    "        aug_min=5,\n",
    "        aug_max=20, \n",
    "        stopwords=stopwords,\n",
    "        stopwords_regex=stopwords_regex\n",
    "    )\n",
    "    character_aug = nafc.Sequential([nac.RandomCharAug(action=\"insert\"),nac.RandomCharAug(action=\"swap\"),nac.RandomCharAug(action=\"delete\")])\n",
    "    augswap = naw.RandomWordAug(action=\"swap\")\n",
    "    swap_sym_word_aug = nafc.Sequential([augswap,augsim])\n",
    "    delete_aug = naw.RandomWordAug()\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=\"baseline-training-data/\"):\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if \"text\" in key:\n",
    "                body = get_text(src_bucket, key)\n",
    "                embedded = embed_text(model_text,tokenizer,body)\n",
    "                new_key_infix = key.split(\"/\")[1].split(\".\")[0]\n",
    "                new_key = dest_prefix + new_key_infix + \".txt\"\n",
    "                copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=new_key, CopySource=copy_source_text)\n",
    "                while True:\n",
    "                    text_augmented = character_aug.augment(body)\n",
    "                    embedded_aug =  embed_text(model_text,tokenizer,text_augmented[0])\n",
    "                    sim = np.dot(embedded, embedded_aug)\n",
    "                    if (sim <= 0.99 and sim >= 0.9):\n",
    "                        augment_key = dest_prefix + new_key_infix + \"_chrarcter_aug\" + \".txt\"\n",
    "                        s3.put_object(Bucket=dest_bucket,\n",
    "                                      Key=augment_key,\n",
    "                                      Body=text_augmented[0].encode(\"utf-8\"),\n",
    "                                      ContentType=\"text/plain\",)\n",
    "                        break\n",
    "\n",
    "\n",
    "                while True:\n",
    "                    text_augmented = delete_aug(body)\n",
    "                    embedded_aug =  embed_text(model_text,tokenizer,text_augmented[0])\n",
    "                    sim = np.dot(embedded, embedded_aug)\n",
    "                    if (sim <= 0.99 and sim >= 0.9):\n",
    "                        augment_key = dest_prefix + new_key_infix + \"_delete_aug\" + \".txt\"\n",
    "                        s3.put_object(Bucket=dest_bucket,\n",
    "                                      Key=augment_key,\n",
    "                                      Body=text_augmented.encode(\"utf-8\"),\n",
    "                                      ContentType=\"text/plain\",)\n",
    "                        break\n",
    "\n",
    "\n",
    "                while True:\n",
    "                    text_augmented = swap_sym_word_aug.augment(body)\n",
    "                    embedded_aug =  embed_text(model_text,tokenizer,text_augmented[0])\n",
    "                    sim = np.dot(embedded, embedded_aug)\n",
    "                    if (sim <= 0.99 and sim >= 0.9):\n",
    "                        augment_key = dest_prefix + new_key_infix + \"_swap_sym_word_aug\" + \".txt\"\n",
    "                        s3.put_object(Bucket=dest_bucket,\n",
    "                                      Key=augment_key,\n",
    "                                      Body=text_augmented.encode(\"utf-8\"),\n",
    "                                      ContentType=\"text/plain\",)\n",
    "                        break\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ca68a8f-e457-433c-a53c-f6e40b3bbcf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_bucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining-data-construction-zone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_bucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maugmentation-zone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 40\u001b[0m, in \u001b[0;36mtext_augmentation\u001b[1;34m(src_bucket, dest_bucket, model_text, tokenizer, dest_prefix)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (sim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m):\n\u001b[0;32m     37\u001b[0m         augment_key \u001b[38;5;241m=\u001b[39m dest_prefix \u001b[38;5;241m+\u001b[39m new_key_infix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_chrarcter_aug\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m         s3\u001b[38;5;241m.\u001b[39mput_object(Bucket\u001b[38;5;241m=\u001b[39mdest_bucket,\n\u001b[0;32m     39\u001b[0m                       Key\u001b[38;5;241m=\u001b[39maugment_key,\n\u001b[1;32m---> 40\u001b[0m                       Body\u001b[38;5;241m=\u001b[39m\u001b[43mtext_augmented\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m                       ContentType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/plain\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "text_augmentation(src_bucket = \"training-data-construction-zone\", dest_bucket = \"augmentation-zone\", model_text = model, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
