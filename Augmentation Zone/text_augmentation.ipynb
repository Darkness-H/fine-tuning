{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1513906-5c7c-4525-abc1-0067f560c81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/60/90/81ac364ef94209c100e12579629dc92bf7a709a84af32f8c551b02c07e94/nltk-3.9.2-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\multi-modal-management-pipeline\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.7 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install nlpaug\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eba5bec-aa97-45d6-ad6a-492a2e432f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import boto3\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "import re\n",
    "from nlpaug.util import Action\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6150ca19-7cf1-4cba-b9ff-ffd20cfd4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053b671e-4df8-4153-a454-cd1e1000b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'augmentation-zone' already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '187915C0DA2AB6D2',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'AAAAAA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '187915C0DA2AB6D2',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '2110',\n",
       "   'x-ratelimit-remaining': '2110',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Tue, 18 Nov 2025 11:17:27 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       " 'ChecksumCRC32': 'AAAAAA==',\n",
       " 'ChecksumType': 'FULL_OBJECT'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new Bucket in Min-IO to store our augmented data\n",
    "\n",
    "# List existing buckets\n",
    "buckets = [b[\"Name\"] for b in s3.list_buckets()[\"Buckets\"]]\n",
    "\n",
    "# Function that given a name, creates a bucket\n",
    "def createBucket(name, list_buckets):\n",
    "    if name in list_buckets:\n",
    "        print(f\"Bucket '{name}' already exists!\")\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=name)\n",
    "        print(f\"Created bucket: {name}\")\n",
    "\n",
    "# Create a bucket named landing_zone\n",
    "createBucket(\"augmentation-zone\", buckets)\n",
    "# Sub-bucket: Baseline Training Data\n",
    "s3.put_object(Bucket=\"augmentation-zone\", Key=\"texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf5e4b2-bfd7-43e9-919e-ecf30285e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddd73f5-2f19-427f-8e3f-ae53f84a259b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, _, _ = open_clip.create_model_and_transforms(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\")\n",
    "tokenizer = open_clip.get_tokenizer(\"hf-hub:laion/CLIP-ViT-L-14-laion2B-s32B-b82K\") # Tokenizer for texts\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d6eedd-e573-42a8-8ccf-d449c481ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf3662ff-d583-4b72-9eba-bc5552acb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate an augmented version of original text and save the reuslt to S3\n",
    "def generate_and_save_augmented(\n",
    "    aug,\n",
    "    suffix: str,\n",
    "    body: str,\n",
    "    embedded_orig: np.ndarray,\n",
    "    model_text,\n",
    "    tokenizer,\n",
    "    dest_bucket: str,\n",
    "    dest_prefix: str,\n",
    "    new_key_infix: str,\n",
    "    max_try: int = 10,\n",
    "    min_sim_accept: float = 0.9,\n",
    "    max_sim_accept: float = 0.99,\n",
    "):\n",
    "    best_text = None\n",
    "    best_sim = -1.0\n",
    "\n",
    "    for attempt in range(max_try):\n",
    "        # 1. Generate augmented text\n",
    "        #    For a single string input, aug.augment(...) returns a string.\n",
    "        aug_text = aug.augment(body)\n",
    "\n",
    "        # 2. Compute cosine similarity with the original text embedding\n",
    "        emb_aug = embed_text(model_text, tokenizer, aug_text)\n",
    "        sim = float(np.dot(embedded_orig, emb_aug))\n",
    "        print(f\"[{suffix}] attempt {attempt + 1}, sim={sim:.4f}\")\n",
    "\n",
    "        # Track the best candidate regardless of whether it is in the range\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_text = aug_text\n",
    "\n",
    "        # If similarity is within the acceptable range, save and return early\n",
    "        if min_sim_accept <= sim <= max_sim_accept:\n",
    "            augment_key = f\"{dest_prefix}{new_key_infix}_{suffix}.txt\"\n",
    "            s3.put_object(\n",
    "                Bucket=dest_bucket,\n",
    "                Key=augment_key,\n",
    "                Body=aug_text.encode(\"utf-8\"),\n",
    "                ContentType=\"text/plain\",\n",
    "            )\n",
    "            return best_sim\n",
    "\n",
    "    # If no candidate falls into [min_sim_accept, max_sim_accept],\n",
    "    # use the best candidate found as a fallback.\n",
    "    if best_text is not None:\n",
    "        augment_key = f\"{dest_prefix}{new_key_infix}_{suffix}.txt\"\n",
    "        print(f\"[{suffix}] use best_sim={best_sim:.4f} as fallback\")\n",
    "        s3.put_object(\n",
    "            Bucket=dest_bucket,\n",
    "            Key=augment_key,\n",
    "            Body=best_text.encode(\"utf-8\"),\n",
    "            ContentType=\"text/plain\",\n",
    "        )\n",
    "\n",
    "    return best_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c5bc29d-ef67-4156-a00e-aa30dc3c7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_augmentation(src_bucket, dest_bucket, model_text, tokenizer, dest_prefix=\"texts/\"):\n",
    "    # Incremental id assigned to each image-text pair\n",
    "    id_counter = 0\n",
    "    stopwords = [ 'Epics', 'Steam'] # don't change name of plataform\n",
    "    stopwords_regex = r'\\d+(\\.\\d+)?'  # don't change numbers\n",
    "    augsim = naw.SynonymAug(\n",
    "        aug_src='wordnet',\n",
    "        lang='eng',\n",
    "        aug_p=0.3,\n",
    "        aug_min=5,\n",
    "        aug_max=20, \n",
    "        stopwords=stopwords,\n",
    "        stopwords_regex=stopwords_regex\n",
    "    )\n",
    "    \n",
    "    spelling_aug = naw.SpellingAug()\n",
    "    augswap = naw.RandomWordAug(action=\"swap\")\n",
    "    swap_sym_word_aug = nafc.Sequential([augswap,augsim])\n",
    "    delete_aug = naw.RandomWordAug()\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=\"baseline-training-data/\"):\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if \"text\" in key:\n",
    "                body = get_text(src_bucket, key)\n",
    "                embedded = embed_text(model_text,tokenizer,body)\n",
    "                new_key_infix = key.split(\"/\")[1].split(\".\")[0]\n",
    "                new_key = dest_prefix + new_key_infix + \".txt\"\n",
    "                copy_source_text = {\"Bucket\": src_bucket, \"Key\": key}\n",
    "                s3.copy_object(Bucket=dest_bucket, Key=new_key, CopySource=copy_source_text)\n",
    "                # 1) word-level insert spelling error\n",
    "                generate_and_save_augmented(\n",
    "                    aug=spelling_aug,\n",
    "                    suffix=\"spelling_aug\",\n",
    "                    body=body,\n",
    "                    embedded_orig=embedded,\n",
    "                    model_text=model_text,\n",
    "                    tokenizer=tokenizer,\n",
    "                    dest_bucket=dest_bucket,\n",
    "                    dest_prefix=dest_prefix,\n",
    "                    new_key_infix=new_key_infix,\n",
    "                    max_try=10,\n",
    "                    min_sim_accept=0.9,\n",
    "                    max_sim_accept=0.99,\n",
    "                )\n",
    "    \n",
    "                # 2) Word-level delete\n",
    "                generate_and_save_augmented(\n",
    "                    aug=delete_aug,\n",
    "                    suffix=\"delete_aug\",\n",
    "                    body=body,\n",
    "                    embedded_orig=embedded,\n",
    "                    model_text=model_text,\n",
    "                    tokenizer=tokenizer,\n",
    "                    dest_bucket=dest_bucket,\n",
    "                    dest_prefix=dest_prefix,\n",
    "                    new_key_infix=new_key_infix,\n",
    "                    max_try=10,\n",
    "                    min_sim_accept=0.9,\n",
    "                    max_sim_accept=0.99,\n",
    "                )\n",
    "    \n",
    "                # 3) Word-level swap + synonym\n",
    "                generate_and_save_augmented(\n",
    "                    aug=swap_sym_word_aug,\n",
    "                    suffix=\"swap_sym_word_aug\",\n",
    "                    body=body,\n",
    "                    embedded_orig=embedded,\n",
    "                    model_text=model_text,\n",
    "                    tokenizer=tokenizer,\n",
    "                    dest_bucket=dest_bucket,\n",
    "                    dest_prefix=dest_prefix,\n",
    "                    new_key_infix=new_key_infix,\n",
    "                    max_try=10,\n",
    "                    min_sim_accept=0.9,\n",
    "                    max_sim_accept=0.99,\n",
    "                )\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ca68a8f-e457-433c-a53c-f6e40b3bbcf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_bucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining-data-construction-zone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_bucket\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maugmentation-zone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 34\u001b[0m, in \u001b[0;36mtext_augmentation\u001b[1;34m(src_bucket, dest_bucket, model_text, tokenizer, dest_prefix)\u001b[0m\n\u001b[0;32m     32\u001b[0m s3\u001b[38;5;241m.\u001b[39mcopy_object(Bucket\u001b[38;5;241m=\u001b[39mdest_bucket, Key\u001b[38;5;241m=\u001b[39mnew_key, CopySource\u001b[38;5;241m=\u001b[39mcopy_source_text)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 1) word-level insert spelling error\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mgenerate_and_save_augmented\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43maug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspelling_aug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspelling_aug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedded_orig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdest_bucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest_bucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdest_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_key_infix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_key_infix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_try\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_sim_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sim_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 2) Word-level delete\u001b[39;00m\n\u001b[0;32m     50\u001b[0m generate_and_save_augmented(\n\u001b[0;32m     51\u001b[0m     aug\u001b[38;5;241m=\u001b[39mdelete_aug,\n\u001b[0;32m     52\u001b[0m     suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete_aug\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     max_sim_accept\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m     63\u001b[0m )\n",
      "Cell \u001b[1;32mIn[48], line 25\u001b[0m, in \u001b[0;36mgenerate_and_save_augmented\u001b[1;34m(aug, suffix, body, embedded_orig, model_text, tokenizer, dest_bucket, dest_prefix, new_key_infix, max_try, min_sim_accept, max_sim_accept)\u001b[0m\n\u001b[0;32m     22\u001b[0m aug_text \u001b[38;5;241m=\u001b[39m aug\u001b[38;5;241m.\u001b[39maugment(body)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 2. Compute cosine similarity with the original text embedding\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m emb_aug \u001b[38;5;241m=\u001b[39m \u001b[43membed_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mdot(embedded_orig, emb_aug))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, sim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36membed_text\u001b[1;34m(model, tokenizer, texts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# The next function returns the embedding of the given text\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_text\u001b[39m(model, tokenizer, texts: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# tokenized batch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     feats \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_text(tokens)\n\u001b[0;32m      6\u001b[0m     feats \u001b[38;5;241m=\u001b[39m feats \u001b[38;5;241m/\u001b[39m feats\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# normalize\u001b[39;00m\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:256\u001b[0m, in \u001b[0;36mSimpleTokenizer.__call__\u001b[1;34m(self, texts, context_length)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# use reduction strategy for tokenize if set, otherwise default to truncation below\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_fn(\n\u001b[0;32m    249\u001b[0m         texts,\n\u001b[0;32m    250\u001b[0m         context_length\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         encode_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode,\n\u001b[0;32m    254\u001b[0m     )\n\u001b[1;32m--> 256\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_token_id] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meot_token_id] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens):\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:215\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    214\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 215\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    217\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:85\u001b[0m, in \u001b[0;36m_clean_lower\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_clean_lower\u001b[39m(x):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# basic, remove whitespace, lower case\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\open_clip\\tokenizer.py:67\u001b[0m, in \u001b[0;36mbasic_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbasic_clean\u001b[39m(text):\n\u001b[1;32m---> 67\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mftfy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mD:\\multi-modal-management-pipeline\\.venv\\Lib\\site-packages\\ftfy\\__init__.py:349\u001b[0m, in \u001b[0;36mfix_text\u001b[1;34m(text, config, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[1;32m--> 349\u001b[0m     textbreak \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pos) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m textbreak \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    351\u001b[0m         textbreak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "text_augmentation(src_bucket = \"training-data-construction-zone\", dest_bucket = \"augmentation-zone\", model_text = model, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
